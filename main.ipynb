{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRv8adCyfT6f"
      },
      "source": [
        "# **Telecom Machine Learning Project to Predict Customer Churn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9kuB3TCzJwB"
      },
      "outputs": [],
      "source": [
        "# Colab mount only when running in Google Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    pass  # No-op on local environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXZnQ0Q5eo-e"
      },
      "source": [
        "## **Business Overview**\n",
        "\n",
        "The telecommunications industry is a rapidly growing sector that is constantly evolving to meet the demands of consumers. As technology advances and user behavior changes, telecom operators face a variety of challenges that can impact their business success. In order to stay competitive and meet customer needs, it is important for telecom companies to regularly analyze their data to identify relevant problems and opportunities for improvement.\n",
        "\n",
        "\n",
        "**Aim:**\n",
        "\n",
        "The aim of a churn prediction notebook is to develop a machine learning model that can predict which customers are likely to churn or discontinue their use of a service or product. Churn prediction is a critical business problem for companies that operate on a subscription or recurring revenue model, such as telecommunications companies.\n",
        "\n",
        "While the project will involve building a churn prediction model, the primary focus will be on the importance of monitoring and adapting to changes in the data that may affect the accuracy and effectiveness of the model over time. The project will also emphasize the need for a feedback loop that allows for continuous improvement and refinement of the model based on new data and changing business requirements. By highlighting these concepts, the project aims to help businesses understand the importance of staying agile and adaptable in their machine learning approaches, rather than solely focusing on the accuracy of a single model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yaKuQlUUooQ"
      },
      "source": [
        "\n",
        "\n",
        "**What is Churn Prediction?**\n",
        "\n",
        "Churn prediction is the process of identifying customers who are likely to discontinue using a service or product. In the context of the telecom sector, churn prediction is the process of identifying customers who are likely to switch to a competitor or terminate their contract with their current service provider.\n",
        "\n",
        "Churn prediction is a critical problem in the telecom sector, as it has a significant impact on a service provider's revenue and profitability. The telecom sector is highly competitive, and service providers are constantly vying for customers' attention. It helps service providers identify customers who are at risk of leaving and take proactive measures to retain them.\n",
        "\n",
        "**Challenges**\n",
        "\n",
        "* Churn prediction is a challenging problem, as it involves analyzing large volumes of data from multiple sources. Telecom service providers generate a vast amount of data from customer interactions, network performance, and billing systems. This data is typically stored in disparate systems, making it difficult to analyze and derive insights.\n",
        "\n",
        "* Another challenge in churn prediction is the diversity of customer behavior. Customers have different reasons for leaving a service provider, and these reasons can be difficult to predict. Some customers may leave due to poor network performance, while others may switch to a competitor offering a better deal. Predicting churn accurately requires understanding these different customer behaviors and identifying the most critical predictors of churn.\n",
        "\n",
        "**Business Impact of Churn Prediction**\n",
        "\n",
        "Churn prediction has a significant impact on a telecom service provider's business. A high churn rate can result in a loss of revenue and profitability. On the other hand, an effective churn prediction model can help service providers identify customers who are at risk of leaving and take proactive measures to retain them.\n",
        "\n",
        "Here are some of the key business impacts of churn prediction:\n",
        "\n",
        "* **Revenue Protection**: Churn prediction helps service providers protect their revenue by identifying customers who are likely to leave and taking proactive measures to retain them. This can include offering discounts, upgrading service plans, or providing additional services. By retaining customers, service providers can maintain their revenue stream and avoid the cost of acquiring new customers.\n",
        "\n",
        "* **Customer Retention**: Churn prediction helps service providers retain their existing customers by identifying their needs and preferences. By understanding why customers leave, service providers can make improvements to their service and provide a better customer experience. This can help to build customer loyalty and increase the lifetime value of a customer.\n",
        "\n",
        "* **Cost Reduction**: Churn prediction can help service providers reduce the cost of acquiring new customers. Acquiring new customers is more expensive than retaining existing ones, and churn prediction can help service providers focus their marketing efforts on the most valuable customers.\n",
        "\n",
        "* **Competitive Advantage**: Churn prediction can provide a significant competitive advantage in the telecom sector. By retaining customers and improving the customer experience, service providers can differentiate themselves from their competitors. This can help to increase market share and profitability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdkeckhCKFMo"
      },
      "source": [
        "## **Approach**\n",
        "\n",
        "**Data exploration**\n",
        "\n",
        "* Load the dataset and examine its structure and contents.\n",
        "* Explore the distribution of the target variable (churn) and the features.\n",
        "\n",
        "**Data preprocessing**\n",
        "\n",
        "* Handle missing values by imputing them with appropriate values.\n",
        "* Handle outliers by removing or transforming them.\n",
        "* Encode categorical variables using one-hot encoding.\n",
        "* Scale numerical variables using Standard scaler.\n",
        "\n",
        "\n",
        "**Model training**\n",
        "\n",
        "* Split the data into training and validation sets.\n",
        "* Train logistic regression, random forest, and XGBoost models on the training set.\n",
        "* Evaluate the performance of the models on the validation set using metrics such as accuracy, precision, recall, and F1 score.\n",
        "* Choose the best-performing model based on the evaluation results.\n",
        "\n",
        "**Data drift monitoring**\n",
        "\n",
        "* Use deep checks to monitor for data drift in the input features and the target variable.\n",
        "* Check the model's performance on the validation set regularly to detect any model drift.\n",
        "\n",
        "**Inference pipeline**\n",
        "\n",
        "* Build an inference pipeline to predict churn for new data.\n",
        "* Handle cases where the label (churn) is not present in the input data.\n",
        "* Handle cases where the drift is detected by retraining the model with misclassified data.\n",
        "\n",
        "**Project Summary**\n",
        "\n",
        "* Summarize the results and draw insights from the model's predictions.\n",
        "* Provide recommendations for business actions based on the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0k5VMe9LRWn"
      },
      "source": [
        "## **Learning Outcomes**\n",
        "\n",
        "* Understanding the problem of customer churn and its impact on businesses.\n",
        "\n",
        "* Understanding the importance of data cleaning and preprocessing in building accurate machine learning models.\n",
        "* Learning how to handle missing values, outliers, and categorical variables using one-hot encoding and numerical variables using standard scaler.\n",
        "* Understanding different machine learning algorithms such as logistic regression, random forest, and XGBoost, and their pros and cons in predicting churn.\n",
        "* Learning how to evaluate machine learning models using metrics such as accuracy, precision, recall, and F1 score.\n",
        "* Understanding the importance of monitoring data drift in machine learning models and how to use deep checks to detect it.\n",
        "* Learning how to build an inference pipeline for predicting churn for new data.\n",
        "* Understanding the limitations of the model and the potential impact of false positives and false negatives.\n",
        "* Learning how to provide recommendations for business actions based on the model's predictions, such as targeted marketing campaigns and retention strategies.\n",
        "* Understanding the iterative nature of machine learning and the need for continuous improvement and retraining of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdQ82V3M_-R"
      },
      "source": [
        "## **Package Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KU4tEjC15Jr"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qAwTZ6tM-5k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlp82fzIY6xx"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Dj_tQGTTkB"
      },
      "source": [
        "After package installation, kindly restart the runtime if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbG5fh1i9Q3U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-I2Xd4o4uKB"
      },
      "source": [
        "## **Data Reading from Different Sources**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ffu_ikbNanX"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here to view more</summary>\n",
        "\n",
        "\n",
        "### **Files**\n",
        "\n",
        "In many cases, the data is stored in the local system. To read the data from the local system, specify the correct path and filename.\n",
        "\n",
        "### **CSV Format**\n",
        "Comma-separated values, also known as CSV, is a specific way to store data in a table structure format. The data used in this project is stored in a CSV file. Download the data for the project.\n",
        "\n",
        "\n",
        "Use following code to read data from csv file using pandas.\n",
        "```\n",
        "import pandas as pd\n",
        "csv_file_path= \"D:/ProjectPro/Telecom Machine Learning Project to Predict Customer Churn/data/Telecom_data.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "```\n",
        "With appropriate csv_file_path, pd.read_csv() function will read the data and store it in df variable.\n",
        "\n",
        "If you get *FileNotFoundError or No such file or directory*, try checking the path provided in the function. It's possible that python is not able to find the file or directory at a given location.\n",
        "\n",
        "\n",
        "### **Colab - CSV Format**\n",
        "\n",
        "```\n",
        "# mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "csv_file_path= '/content/drive/MyDrive/project_pro/Telecom Machine Learning Project to Predict Customer Churn/Telecom_data.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "```\n",
        "\n",
        "### **AWS S3 - CSV**\n",
        "Use the S3 public link to read the CSV file directly into a pandas DataFrame\n",
        "```\n",
        "s3_link = 'https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv'\n",
        "df = pd.read_csv(s3_link)\n",
        "```\n",
        "\n",
        "### **Database**\n",
        "Most organizations store their data in databases such as MS SQL. Microsoft SQL Server (MS SQL) is a relational database management system developed by Microsoft. A BAK file in Microsoft SQL Server is a backup file that contains a copy of a SQL Server database at a specific point in time. It is essentially a binary representation of the database and includes all its data, tables, schema, indexes, stored procedures, and other objects.\n",
        "\n",
        "#### **Installing MS SQL Management Studio**\n",
        "To install Microsoft SQL Server Management Studio, you can follow these steps:\n",
        "\n",
        "* Go to the Microsoft SQL Server Downloads page (https://www.microsoft.com/en-us/sql-server/sql-server-downloads) and click on the \"Download now\" button for the version of SQL Server Management Studio that you want to install.\n",
        "* Follow the instructions on the screen to download the installation file to your computer.\n",
        "* Once the download is complete, locate the installation file and double-click on it to start the installation process.\n",
        "\n",
        "\n",
        "#### **Restore a BAK file in MS SQL**\n",
        "\n",
        "* Open SQL Server Management Studio and connect to the SQL Server instance to which you want to upload the BAK file.\n",
        "* Right-click on the Databases folder in the Object Explorer pane and select \"Restore Database...\" from the context menu.\n",
        "* In the \"Restore Database\" dialog box, select the \"Device\" option under the \"Source\" section.\n",
        "* Click on the \"...\" button to open the \"Select backup devices\" dialog box.\n",
        "In the \"Select backup devices\" dialog box, click on the \"Add\" button to add the BAK file that you want to upload.\n",
        "* In the \"Locate Backup File\" dialog box, browse to the location where the BAK file is stored in the Telecom Data Analysis Project to Improve Service Quality directory under the ‘data’ folder, select the file, and click on the \"OK\" button.\n",
        "* Back in the \"Select backup devices\" dialog box, the BAK file you added should now be listed under \"Backup media:\".\n",
        "* Click on the \"OK\" button to close the \"Select backup devices\" dialog box.\n",
        "In the \"Restore Database\" dialog box, you should see the BAK file listed in the \"Backup sets to restore\" section.\n",
        "* By default, the original database name and file locations from the BAK file will be used. If you want to restore the database with a different name or to a different location, you can modify the \"To database\" and \"Restore as\" options under the \"General\" section.\n",
        "* Click the \"Options\" tab for additional restore options.\n",
        "* If you want to overwrite an existing database with the same name, you can select the \"Overwrite the existing database (WITH REPLACE)\" option under the \"Restore options\" section.\n",
        "* Click on the \"OK\" button to start the restore process.\n",
        "* Once the restore process is complete, you should see a message indicating that the restore was successful.\n",
        "\n",
        "#### **Read Data from DB to Python**\n",
        " The data can be accessed by secret credentials, which will be in the following format.\n",
        "```\n",
        "import pyodbc\n",
        "import pandas as pd\n",
        "connection = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};\\\n",
        "                       SERVER=server_name;\\\n",
        "                       PORT=1433;\\\n",
        "                       DATABASE=database_name;\\\n",
        "                       UID=admin;\\\n",
        "                       PWD=password')\n",
        "```\n",
        "#### **Steps to install ODBC driver**\n",
        "* Go to the Microsoft Download Center page for the ODBC Driver 17 for SQL Server: https://www.microsoft.com/en-us/download/details.aspx?id=56567\n",
        "* Select the download button that corresponds to the operating system you are using.\n",
        "* Select the language you want to use for the installer, then click the download button.\n",
        "* Once the download is complete, run the installer.\n",
        "* Accept the license terms, then select the features you want to install.\n",
        "Choose a location to install the driver, or use the default location.\n",
        "* Complete the installation process by following the instructions provided by the installer.\n",
        "* Once the installation is complete, you can use the ODBC Driver 17 for SQL Server to connect to SQL Server databases from applications that support ODBC connectivity.\n",
        "\n",
        "#### **Query to read the data into Pandas**\n",
        "\n",
        "```\n",
        "\n",
        "query = '''select * from Processed_month_1_data\n",
        "           UNION ALL\n",
        "            select * from Processed_month_2_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_3_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_4_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_5_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_6_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_7_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_8_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_9_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_10_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_11_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_12_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_13_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_14_data'''\n",
        "processed_data = pd.read_sql(query,connection)\n",
        "processed_data.head()\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cGA-Er4Z-2i"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WNyymnhZ-2i"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-izhMtyrZ-2j"
      },
      "outputs": [],
      "source": [
        "# Read data: prefer local CSV if available, else use S3 public link\n",
        "import os\n",
        "local_paths = [\n",
        "    'Telecom_Data.csv',\n",
        "    './data/Telecom_Data.csv',\n",
        "]\n",
        "s3_link = 'https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv'\n",
        "chosen = None\n",
        "for p in local_paths:\n",
        "    if os.path.exists(p):\n",
        "        chosen = p\n",
        "        break\n",
        "if chosen is None:\n",
        "    chosen = s3_link\n",
        "print(f'Reading data from: {chosen}')\n",
        "df = pd.read_csv(chosen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TbXaeS31ttY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE7o3wt81ttZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fqx0MsIZ-2j"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the Dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMm321OAZ-2k"
      },
      "outputs": [],
      "source": [
        "# Check the Information of the Dataframe, datatypes and non-null counts\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyb0dMULZ-2k"
      },
      "outputs": [],
      "source": [
        "# Checking the names of the columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXTBcfTrORWl"
      },
      "source": [
        "## **Data Dictionary**\n",
        "\n",
        "\n",
        "\n",
        "| Column name\t | Description|\n",
        "| ----- | ----- |\n",
        "| Customer ID\t | Unique identifier for each customer |\n",
        "| Month | Calendar Month- 1:12 |\n",
        "| Month of Joining |\tCalender Month -1:14, Month for which the data is captured|\n",
        "| zip_code |\tZip Code|\n",
        "|Gender |\tGender|\n",
        "| Age |\tAge(Years)|\n",
        "| Married |\tMarital Status |\n",
        "|Dependents | Dependents - Binary |\n",
        "| Number of Dependents |\tNumber of Dependents|\n",
        "|Location ID |\tLocation ID|\n",
        "|Service ID\t |Service ID|\n",
        "|state|\tState|\n",
        "|county\t|County|\n",
        "|timezone\t|Timezone|\n",
        "|area_codes|\tArea Code|\n",
        "|country\t|Country|\n",
        "|latitude|\tLatitude|\n",
        "|longitude\t|Longitude|\n",
        "|arpu|\tAverage revenue per user|\n",
        "|roam_ic\t|Roaming incoming calls in minutes|\n",
        "|roam_og\t|Roaming outgoing calls in minutes|\n",
        "|loc_og_t2t|\tLocal outgoing calls within same network in minutes|\n",
        "|loc_og_t2m\t|Local outgoing calls outside network in minutes(outside same + partner network)|\n",
        "|loc_og_t2f|\tLocal outgoing calls with Partner network in minutes|\n",
        "|loc_og_t2c\t|Local outgoing calls with Call Center in minutes|\n",
        "|std_og_t2t|\tSTD outgoing calls within same network in minutes|\n",
        "|std_og_t2m|\tSTD outgoing calls outside network in minutes(outside same + partner network)|\n",
        "|std_og_t2f|\tSTD outgoing calls with Partner network in minutes|\n",
        "|std_og_t2c\t|STD outgoing calls with Call Center in minutes|\n",
        "|isd_og|\tISD Outgoing calls|\n",
        "|spl_og\t|Special Outgoing calls|\n",
        "|og_others|\tOther Outgoing Calls|\n",
        "|loc_ic_t2t|\tLocal incoming calls within same network in minutes|\n",
        "|loc_ic_t2m|\tLocal incoming calls outside network in minutes(outside same + partner network)|\n",
        "|loc_ic_t2f\t|Local incoming calls with Partner network in minutes|\n",
        "|std_ic_t2t\t|STD incoming calls within same network in minutes|\n",
        "|std_ic_t2m\t|STD incoming calls outside network in minutes(outside same + partner network)|\n",
        "|std_ic_t2f|\tSTD incoming calls with Partner network in minutes|\n",
        "|std_ic_t2o|\tSTD incoming calls operators other networks in minutes|\n",
        "|spl_ic|\tSpecial Incoming calls in minutes|\n",
        "|isd_ic|\tISD Incoming calls in minutes|\n",
        "|ic_others|\tOther Incoming Calls|\n",
        "|total_rech_amt|\tTotal Recharge Amount in Local Currency|\n",
        "|total_rech_data|\tTotal Recharge Amount for Data in Local Currency\n",
        "|vol_4g|\t4G Internet Used in GB|\n",
        "|vol_5g|\t5G Internet used in GB|\n",
        "|arpu_5g|\tAverage revenue per user over 5G network|\n",
        "|arpu_4g|\tAverage revenue per user over 4G network|\n",
        "|night_pck_user|\tIs Night Pack User(Specific Scheme)|\n",
        "|fb_user|\tSocial Networking scheme|\n",
        "|aug_vbc_5g|\tVolume Based cost for 5G network (outside the scheme paid based on extra usage)|\n",
        "|offer|\tOffer Given to User|\n",
        "|Referred a Friend|\tReferred a Friend : Binary|\n",
        "|Number of Referrals|\tNumber of Referrals|\n",
        "|Phone Service|\tPhone Service: Binary|\n",
        "|Multiple Lines|\tMultiple Lines for phone service: Binary|\n",
        "|Internet Service|\tInternet Service: Binary|\n",
        "|Internet Type|\tInternet Type|\n",
        "|Streaming Data Consumption|\tStreaming Data Consumption|\n",
        "|Online Security|\tOnline Security|\n",
        "|Online Backup|\tOnline Backup|\n",
        "|Device Protection Plan|\tDevice Protection Plan|\n",
        "|Premium Tech Support|\tPremium Tech Support|\n",
        "|Streaming TV|\tStreaming TV|\n",
        "|Streaming Movies|\tStreaming Movies|\n",
        "|Streaming Music|\tStreaming Music|\n",
        "|Unlimited Data|\tUnlimited Data|\n",
        "|Payment Method|\tPayment Method|\n",
        "|Status ID|\tStatus ID|\n",
        "|Satisfaction Score|\tSatisfaction Score|\n",
        "|Churn Category|\tChurn Category|\n",
        "|Churn Reason|\tChurn Reason|\n",
        "|Customer Status|\tCustomer Status|\n",
        "|Churn Value|\tBinary Churn Value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYI9w9ZQZ-2k"
      },
      "outputs": [],
      "source": [
        "# Null values sum\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P-B9TxjZ-2l"
      },
      "outputs": [],
      "source": [
        "# Null values in total recharge data\n",
        "df['total_rech_data'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFPE07KUZ-2l"
      },
      "outputs": [],
      "source": [
        "# Null values in Internet Type\n",
        "df['Internet Type'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20Xrd0RHZ-2l"
      },
      "outputs": [],
      "source": [
        "# Missing value percentage\n",
        "df['total_rech_data'].isna().sum()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmWVmQQXZ-2l"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*  These missing values may represent customers who have not recharged their account or have recharged but the information has not been recorded.\n",
        "\n",
        "* It is possible that customers with missing recharge data are those who received free data service, and therefore did not need to recharge their account. Alternatively, it is possible that the missing values are due to technical issues, such as data recording errors or system failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvV6-BuZZ-2m"
      },
      "outputs": [],
      "source": [
        "# Checking the value counts of Internet Service where total recharge data was null\n",
        "df[df['total_rech_data'].isna()]['Internet Service'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR2RpuGKZ-2m"
      },
      "source": [
        "**Observation**:\n",
        "\n",
        "* It turns out that all customers with missing recharge data have opted for internet service, the next step could be to check if they have used it or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBgsjUoPZ-2m"
      },
      "outputs": [],
      "source": [
        "# Let's check unlimited data column\n",
        "df[(df['total_rech_data'].isna())]['Unlimited Data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVj27H0xZ-2m"
      },
      "outputs": [],
      "source": [
        "# Lets check Average Revenue for 4g and 5g\n",
        "df[(df['total_rech_data'].isna())][['arpu_4g','arpu_5g']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z--ECzlXZ-2m"
      },
      "source": [
        "**Observation**:\n",
        "\n",
        "* We can fill the missing values in the total_rech_data column with 0 when the arpu (Average Revenue Per User) is not applicable. This is because the arpu is a measure of the revenue generated per user, and if it is not applicable, it may indicate that the user is not generating any revenue for the company. In such cases, it is reasonable to assume that the total recharge amount is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKsHAQQcZ-2n"
      },
      "outputs": [],
      "source": [
        "# Check the value counts of ARPU 4g and 5g\n",
        "df[['arpu_4g','arpu_5g']].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHibU4lcZ-2n"
      },
      "outputs": [],
      "source": [
        "# Replacing all values of total recharge data= 0 where arpu 4g and 5g are not applicable\n",
        "df.loc[(df['arpu_4g']=='Not Applicable') | (df['arpu_5g']=='Not Applicable'),'total_rech_data']=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZywhbWfZ-2n"
      },
      "outputs": [],
      "source": [
        "# Missing value percentage\n",
        "df['total_rech_data'].isna().sum()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvS-GOYdZ-2n"
      },
      "source": [
        "We cannot fill other values with 0 because they have some ARPU to consider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xivTAuUZ-2n"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n",
        "df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5NNUYX8Z-2n"
      },
      "source": [
        "With this mean, we will fill the NaN values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePxg16ZkZ-2n"
      },
      "outputs": [],
      "source": [
        "# Fill NaN values in 'total_rech_data' with the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n",
        "df['total_rech_data']=df['total_rech_data'].fillna(df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y8-hQOgZ-2n"
      },
      "outputs": [],
      "source": [
        "# Check the value counts for Internet Type\n",
        "df['Internet Type'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VojpiRlLZ-2o"
      },
      "outputs": [],
      "source": [
        "# Check value counts for Internet Service where Internet Type is null\n",
        "df[df['Internet Type'].isna()]['Internet Service'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDvmPBoeZ-2o"
      },
      "source": [
        "All null values in Internet Type does not have Internet Service. Let's fill these null values with Not Applicable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjmE5bcbZ-2o"
      },
      "outputs": [],
      "source": [
        "# Filling Null values in Internet Type\n",
        "df['Internet Type']=df['Internet Type'].fillna('Not Applicable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoezoPLWZ-2o"
      },
      "outputs": [],
      "source": [
        "# Shape of the dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjO5_qGIZ-2o"
      },
      "outputs": [],
      "source": [
        "# Insert a new column named 'total_recharge' before the last column in the dataframe\n",
        "# The values of 'total_recharge' are the sum of 'total_rech_amt' and 'total_rech_data'\n",
        "df.insert(loc=df.shape[1]-1,column='total_recharge',value=df['total_rech_amt']+df['total_rech_data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkQYshrQZ-2o"
      },
      "outputs": [],
      "source": [
        "# Checking percent of missing values in columns\n",
        "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
        "df_missing_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHK6wY6OZ-2o"
      },
      "source": [
        "Let's drop some unnecessary columns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRayCzDDZ-2o"
      },
      "outputs": [],
      "source": [
        "# Dropping columns\n",
        "df=df.drop(columns=['night_pck_user', 'fb_user','Churn Category','Churn Reason', 'Customer Status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCVlehSCZ-2o"
      },
      "outputs": [],
      "source": [
        "# Checking churn %\n",
        "round(100*(df['Churn Value'].mean()),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d99fGw_Z-2p"
      },
      "outputs": [],
      "source": [
        "# Number of unique latitudes\n",
        "df['latitude'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cHkBum8Z-2p"
      },
      "outputs": [],
      "source": [
        "# Number of unique longitudes\n",
        "df['longitude'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mp5w1niZ-2p"
      },
      "source": [
        "Replace 'Not Applicable' with 0 in both 'arpu_4g' and 'arpu_5g'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74bP-j3tZ-2p"
      },
      "outputs": [],
      "source": [
        "# Replace 'Not Applicable' with 0 in 'arpu_4g'\n",
        "df['arpu_4g'] = df['arpu_4g'].replace('Not Applicable', 0)\n",
        "\n",
        "# Replace 'Not Applicable' with 0 in 'arpu_5g'\n",
        "df['arpu_5g'] = df['arpu_5g'].replace('Not Applicable', 0)\n",
        "\n",
        "# Convert 'arpu_4g' to float data type\n",
        "df['arpu_4g'] = df['arpu_4g'].astype(float)\n",
        "\n",
        "# Convert 'arpu_5g' to float data type\n",
        "df['arpu_5g'] = df['arpu_5g'].astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NypxsZ4JZ-2p"
      },
      "outputs": [],
      "source": [
        "# Check the data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HS3WicKZ-2p"
      },
      "outputs": [],
      "source": [
        "# Note: We are keeping customer location-based attributes aside for now\n",
        "location_att=['zip_code''state', 'county', 'timezone', 'area_codes', 'country','latitude','longitude']\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_cols=['Gender',\n",
        "       'Married', 'Dependents',\n",
        "       'offer','Referred a Friend', 'Phone Service',\n",
        "       'Multiple Lines', 'Internet Service', 'Internet Type',\n",
        "        'Online Security', 'Online Backup',\n",
        "       'Device Protection Plan', 'Premium Tech Support', 'Streaming TV',\n",
        "       'Streaming Movies', 'Streaming Music', 'Unlimited Data',\n",
        "       'Payment Method']\n",
        "\n",
        "# List of continuous columns\n",
        "cts_cols=['Age','Number of Dependents',\n",
        "       'roam_ic', 'roam_og', 'loc_og_t2t',\n",
        "       'loc_og_t2m', 'loc_og_t2f', 'loc_og_t2c', 'std_og_t2t', 'std_og_t2m',\n",
        "       'std_og_t2f', 'std_og_t2c', 'isd_og', 'spl_og', 'og_others',\n",
        "       'loc_ic_t2t', 'loc_ic_t2m', 'loc_ic_t2f', 'std_ic_t2t', 'std_ic_t2m',\n",
        "       'std_ic_t2f', 'std_ic_t2o', 'spl_ic', 'isd_ic', 'ic_others',\n",
        "       'total_rech_amt', 'total_rech_data', 'vol_4g', 'vol_5g', 'arpu_5g',\n",
        "       'arpu_4g', 'arpu', 'aug_vbc_5g', 'Number of Referrals','Satisfaction Score',\n",
        "       'Streaming Data Consumption']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqaJC1SzZ-2p"
      },
      "outputs": [],
      "source": [
        "# Create an empty dataframe with columns as cts_cols and index as quantiles\n",
        "quantile_df=pd.DataFrame(columns=cts_cols,index=[0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])\n",
        "\n",
        "# for each column in cts_cols, calculate the corresponding quantiles and store them in the quantile_df\n",
        "for col in cts_cols:\n",
        "   quantile_df[col]=df[col].quantile([0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRSmmv015Jz"
      },
      "source": [
        "By calculating quantiles for each continuous variable in the dataset, we are trying to get an idea about the spread and distribution of the data. Specifically, we are interested in identifying potential outliers in the data.\n",
        "\n",
        "Quantiles divide a distribution into equal proportions. For instance, the 0.25 quantile is the value below which 25% of the observations fall and the 0.75 quantile is the value below which 75% of the observations fall. By calculating quantiles at various levels, we can get a better understanding of the distribution of the data and identify any observations that are too far away from the rest of the data.\n",
        "\n",
        "These quantiles can be used as thresholds to identify potential outliers in the data. Observations with values beyond these thresholds can be considered as potential outliers and further investigation can be carried out to determine if they are true outliers or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH6bWCegZ-2p"
      },
      "outputs": [],
      "source": [
        "# Let's check out the quantiles df\n",
        "quantile_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqnLtZs1Z-2p"
      },
      "source": [
        "Outliers were detected in the variables vol_5g, arpu_4g, and arpu_5g."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6NQZsd3Z-2q"
      },
      "outputs": [],
      "source": [
        "# Checking further\n",
        "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcx5-kVNZ-2q"
      },
      "outputs": [],
      "source": [
        "# Calculate the proportion of rows in the DataFrame where the value in the 'arpu_4g' column is equal to 254687\n",
        "df[df['arpu_4g']==254687].shape[0]/df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJjw0lS5Z-2q"
      },
      "outputs": [],
      "source": [
        "# Let's check it out\n",
        "df[df['arpu_4g']==254687]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzuy2bvZ-2q"
      },
      "source": [
        "Let's see what is the value of 'total_rech_data' for these observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ8kQmwSZ-2q"
      },
      "outputs": [],
      "source": [
        "# Get the value counts of 'total_rech_data' for observations where the value in the 'arpu_4g' column is equal to 254687\n",
        "df[df['arpu_4g']==254687]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4c454tIZ-2q"
      },
      "source": [
        "Now, since the recharge amount is 0 and there is no ARPU, let's replace it with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akCqNl15Z-2q"
      },
      "outputs": [],
      "source": [
        "# Replace the outlier value 254687 in the 'arpu_4g' column of the dataframe 'df' with 0.\n",
        "df['arpu_4g']=df['arpu_4g'].replace(254687,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2m3EgcRZ-2q"
      },
      "outputs": [],
      "source": [
        "# Checking further\n",
        "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crbcd8PaZ-2q"
      },
      "outputs": [],
      "source": [
        "# Filter by 'arpu_4g' value of 87978 and count unique values in 'total_rech_data' column\n",
        "df[df['arpu_4g']==87978]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jocWPh15J1"
      },
      "source": [
        "All rows in the dataframe with an 'arpu_4g' value of 87978 have 0 value in the 'total_rech_data' column, indicating that these are likely outliers. Therefore, we have decided to replace the 'arpu_4g' value for these rows with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuOjiovGZ-2q"
      },
      "outputs": [],
      "source": [
        "# Replace the values with 0\n",
        "df['arpu_4g']=df['arpu_4g'].replace(87978,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7aizZWaZ-2q"
      },
      "outputs": [],
      "source": [
        "# Checking the quantiles again\n",
        "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJp0H0BRZ-2r"
      },
      "outputs": [],
      "source": [
        "# Check the churn value for this ARPU\n",
        "df[df['arpu_4g']>8000]['Churn Value'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrZP-iq5Z-2r"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        " * A higher ARPU suggests that a business is generating more revenue per user, which can be a positive sign for the business's profitability. However, a high ARPU can also imply churn, or the rate at which customers are leaving the business.\n",
        "\n",
        "* There are a few reasons why a high ARPU may imply churn. First, if a business is charging a high price for its services, it may attract a customer base that is more price-sensitive and likely to switch to a competitor if they find a better deal. This could result in a higher churn rate for the business."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwB9svs2Z-2r"
      },
      "outputs": [],
      "source": [
        "# Check the value counts of total recharge data at outlying values\n",
        "df[df['arpu_5g']==254687]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvnOLN9tZ-2r"
      },
      "outputs": [],
      "source": [
        "# Check the value counts of total recharge data at outlying values\n",
        "df[df['arpu_5g']==87978]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWUCA-0QZ-2r"
      },
      "outputs": [],
      "source": [
        "# Replacing the values with 0 where total recharge data is 0\n",
        "df['arpu_5g']=df['arpu_5g'].replace([87978,254687],0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ2G0_CgZ-2r"
      },
      "outputs": [],
      "source": [
        "# Check the quantiles of ARPU 5G\n",
        "df['arpu_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "288nXt4mZ-2r"
      },
      "outputs": [],
      "source": [
        "# Check the quantiles of Volume of 5G data\n",
        "df['vol_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.98,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk5WDhO4Z-2r"
      },
      "outputs": [],
      "source": [
        "# Lets see the recharge data value\n",
        "df[df['vol_5g']>=87978]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnX_gEMTZ-2r"
      },
      "outputs": [],
      "source": [
        "# Proportion of these values\n",
        "df[df['vol_5g']>=87978]['total_rech_data'].value_counts()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GufyTVuZ-2s"
      },
      "source": [
        "**Observation**:\n",
        "\n",
        "There is a presence of 2% outliers in vol 5g, where the values are very high, but their total recharge data is 0. We will fill these outliers with 0, and below are some possible reasons why this could be:\n",
        "\n",
        "* Data recording error: It is possible that there was an error in recording the recharge data for these outliers, leading to an incorrect value of 0. In this case, it would make sense to fill the outliers with 0, as this is likely the correct value.\n",
        "\n",
        "* Promotions or bonuses: Another possibility is that these customers received promotions or bonuses that allowed them to use the service without recharging, leading to a total recharge data of 0. However, these customers may still be using the service heavily, leading to the high values in vol 5g. In this case, filling the outliers with 0 would make sense as it accurately reflects the lack of recharge data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnSUoP5cZ-2s"
      },
      "outputs": [],
      "source": [
        "# Replace the outlier values\n",
        "df['vol_5g']=df['vol_5g'].replace([87978,254687],0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc4VgnXjZ-2s"
      },
      "outputs": [],
      "source": [
        "# Unique months\n",
        "df['Month'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyuox--FZ-2s"
      },
      "outputs": [],
      "source": [
        "# Unique months of joining\n",
        "df['Month of Joining'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-iiHq7o15J3"
      },
      "outputs": [],
      "source": [
        "# Save Processed data temporarily in Colab\n",
        "df.to_csv('processed_churn_data_1.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCa9t9OFiHtf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VUhYYj7eiMl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQgl0gnaMZz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('processed_churn_data_1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CY0wR2W15J3"
      },
      "source": [
        "### **Quarterly Churn Analysis**\n",
        "\n",
        "Quarterly churn analysis is a process of analyzing the rate at which customers are leaving a business or discontinuing their services over a period of three months. The analysis is usually done by calculating the churn rate, which is the percentage of customers who have stopped using the service during the quarter.\n",
        "\n",
        "* Timeliness: Quarterly churn analysis and prediction allows for a timely assessment of customer retention and churn rates. By conducting this analysis regularly, businesses can identify any changes in customer behavior and take necessary actions to address them in a timely manner.\n",
        "\n",
        "* Evaluation of strategies: Conducting churn analysis and prediction on a quarterly basis enables businesses to evaluate the effectiveness of their customer retention strategies. If the churn rate has increased, the business can evaluate the strategies implemented in the previous quarter and determine whether they were effective or not. This will allow them to adjust their strategies and improve their customer retention efforts.\n",
        "\n",
        "* Financial impact: Churn has a significant financial impact on businesses. By conducting quarterly churn analysis and prediction, businesses can identify areas where they are losing revenue and take steps to prevent further losses. This will help them to maintain financial stability and growth.\n",
        "\n",
        "* Customer insights: Quarterly churn analysis and prediction can also provide valuable insights into customer behavior and preferences. By analyzing customer behavior and reasons for churn, businesses can identify patterns and trends that will help them to improve their services and retain customers in the future.\n",
        "\n",
        "* Benchmarking: Conducting quarterly churn analysis and prediction allows businesses to benchmark their performance against industry standards and competitors. This will help them to identify areas where they are performing well and areas where they need to improve to stay competitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bixkWnVAZ-2s"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Define a function to map a month to its corresponding quarter\n",
        "def map_month_to_quarter(month):\n",
        "    if math.isnan(month): # Handle NaN values if present\n",
        "        return None\n",
        "    quarter = math.ceil(month / 3)\n",
        "    return quarter\n",
        "\n",
        "# Insert a new column called 'Quarter of Joining' in the DataFrame 'df' and populate it with the quarter corresponding to the 'Month of Joining' column\n",
        "df.insert(loc=1,column='Quarter of Joining',value=df['Month of Joining'].apply(lambda x: map_month_to_quarter(x)))\n",
        "\n",
        "# Insert a new column called 'Quarter' in the DataFrame 'df'and populate it with the quarter corresponding to the 'Month' column\n",
        "df.insert(loc=1,column='Quarter',value= df['Month'].apply(lambda x: map_month_to_quarter(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwJBQcGLZ-2t"
      },
      "outputs": [],
      "source": [
        "# Remove duplicate rows in the DataFrame 'df' based on the 'Customer ID', 'Quarter', and 'Quarter of Joining' columns and keep only the last occurrence of each set of duplicates\n",
        "telco=df.drop_duplicates(subset=['Customer ID','Quarter','Quarter of Joining'],keep='last')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8GvS05j15J3"
      },
      "source": [
        "The 'train_data' DataFrame contains the data of customers who joined in the first quarter and were active in the first quarter. This dataset is used for training the churn prediction model.\n",
        "\n",
        "The 'test_data' DataFrame contains the data of customers who joined in the first quarter and were active in the second quarter. This dataset is used for testing the accuracy of the churn prediction model.\n",
        "\n",
        "The 'prediction_data' DataFrame contains the data of customers who joined in the second quarter and were active in the second quarter. This dataset is used for predicting the churn of customers who joined in the second quarter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO88CQjs15KD"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8k_pia3a1KP"
      },
      "outputs": [],
      "source": [
        "df['Churn Value'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGIy42t3Wxq2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Identify columns to drop from features before scaling\n",
        "# 'Customer ID' is a unique identifier and not a feature for the model.\n",
        "# Other object-type columns like 'state', 'county', 'timezone', 'area_codes', 'country' were noted as 'keeping aside for now'.\n",
        "# 'Month' and 'Month of Joining' are already used to create 'Quarter' features, and can be dropped if not needed for ML directly.\n",
        "# 'zip_code', 'Location ID', 'Service ID', 'Status ID' are numerical identifiers, but not direct features, and can also be dropped.\n",
        "\n",
        "cols_to_drop = [\n",
        "    'Customer ID', 'Month', 'Month of Joining', 'zip_code', 'Location ID', 'Service ID',\n",
        "    'state', 'county', 'timezone', 'area_codes', 'country', 'Status ID'\n",
        "]\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# Drop the target variable 'Churn Value' and the identified columns from X\n",
        "X_df = df.drop(columns=['Churn Value'] + cols_to_drop, errors='ignore')\n",
        "y = df['Churn Value']\n",
        "\n",
        "# List of categorical columns (copied from cell -HS3WicKZ-2p)\n",
        "categorical_cols=['Gender',\n",
        "       'Married', 'Dependents',\n",
        "       'offer','Referred a Friend', 'Phone Service',\n",
        "       'Multiple Lines', 'Internet Service', 'Internet Type',\n",
        "        'Online Security', 'Online Backup',\n",
        "       'Device Protection Plan', 'Premium Tech Support', 'Streaming TV',\n",
        "       'Streaming Movies', 'Streaming Music', 'Unlimited Data',\n",
        "       'Payment Method']\n",
        "\n",
        "\n",
        "# Identify categorical columns that are still of 'object' dtype in X_df\n",
        "# Use the predefined 'categorical_cols' from the notebook and ensure they exist in X_df.\n",
        "categorical_features_for_ohe = [col for col in categorical_cols if col in X_df.columns]\n",
        "\n",
        "# Apply one-hot encoding to the identified categorical features\n",
        "X_processed = pd.get_dummies(X_df, columns=categorical_features_for_ohe, drop_first=True)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpK-XFHZWxuP"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWRGtbX4hLRu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model_ann = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history_ann = model_ann.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                            epochs=10, batch_size=32, callbacks=[es])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZVKnTRB2C5t"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "loss, accuracy = model_ann.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR6_pfxFfr-g"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = (model_ann.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcZTgorrgo6N"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "# Reshape input to 3D for Conv1D: (samples, features, 1)\n",
        "X_train_cnn = np.expand_dims(X_train, axis=2)\n",
        "X_test_cnn = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "model_cnn = Sequential([\n",
        "    Conv1D(128, 2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv1D(64, 2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "history_cnn = model_cnn.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test),\n",
        "                            epochs=2, batch_size=32, callbacks=[es])\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43GQg8pqDcSL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def build_transformer(input_shape, num_heads=4, key_dim=32):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "input_shape = X_train.shape[1:]\n",
        "model_transformer = build_transformer(input_shape)\n",
        "\n",
        "# Callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train\n",
        "history_transformer = model_transformer.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "# Save for deployment\n",
        "model_transformer.save(\"transformer_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pztX1K_61ttw"
      },
      "outputs": [],
      "source": [
        "# Install pytorch-tabnet if not already installed\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "except ImportError:\n",
        "    import sys\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytorch-tabnet\"])\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyLlw4z71ttw"
      },
      "outputs": [],
      "source": [
        "# Build and train TabNet model\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "\n",
        "# TabNet requires numpy arrays\n",
        "X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
        "X_test_np = X_test.values if hasattr(X_test, 'values') else X_test\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "y_test_np = y_test.values if hasattr(y_test, 'values') else y_test\n",
        "\n",
        "# Initialize TabNet classifier\n",
        "model_tabnet = TabNetClassifier(\n",
        "    n_d=64,                    # Width of the decision prediction layer\n",
        "    n_a=64,                    # Width of the attention embedding for each mask\n",
        "    n_steps=5,                 # Number of steps in the architecture (decision steps)\n",
        "    gamma=1.5,                 # Coefficient for feature reusage in the masks\n",
        "    n_independent=2,           # Number of independent Gated Linear Units layers at each step\n",
        "    n_shared=2,                # Number of shared Gated Linear Units at each step\n",
        "    lambda_sparse=1e-4,        # Sparsity regularization coefficient\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type='entmax',        # Either 'sparsemax' or 'entmax'\n",
        "    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    seed=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model_tabnet.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_name=['test'],\n",
        "    eval_metric=['auc', 'accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=15,\n",
        "    batch_size=256,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model_tabnet.save_model('tabnet_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXIHCAlq1ttw"
      },
      "outputs": [],
      "source": [
        "# Evaluate TabNet model\n",
        "y_pred_tabnet = model_tabnet.predict(X_test_np)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "acc_tabnet = accuracy_score(y_test_np, y_pred_tabnet)\n",
        "prec_tabnet = precision_score(y_test_np, y_pred_tabnet)\n",
        "rec_tabnet = recall_score(y_test_np, y_pred_tabnet)\n",
        "f1_tabnet = f1_score(y_test_np, y_pred_tabnet)\n",
        "\n",
        "# Get prediction probabilities for AUC\n",
        "y_pred_proba_tabnet = model_tabnet.predict_proba(X_test_np)[:, 1]\n",
        "auc_tabnet = roc_auc_score(y_test_np, y_pred_proba_tabnet)\n",
        "\n",
        "print(f\"\\n📊 TabNet Evaluation:\")\n",
        "print(f\"Accuracy: {acc_tabnet:.4f}\")\n",
        "print(f\"Precision: {prec_tabnet:.4f}\")\n",
        "print(f\"Recall: {rec_tabnet:.4f}\")\n",
        "print(f\"F1-Score: {f1_tabnet:.4f}\")\n",
        "print(f\"ROC-AUC: {auc_tabnet:.4f}\")\n",
        "\n",
        "results_tabnet = [acc_tabnet, prec_tabnet, rec_tabnet, f1_tabnet, auc_tabnet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwd2zxOz1ttw"
      },
      "outputs": [],
      "source": [
        "# Visualize TabNet feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from TabNet\n",
        "feature_importances = model_tabnet.feature_importances_\n",
        "\n",
        "# Get feature names (assuming X_train is a DataFrame, otherwise use indices)\n",
        "if hasattr(X_train, 'columns'):\n",
        "    feature_names = X_train.columns\n",
        "else:\n",
        "    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
        "\n",
        "# Sort features by importance\n",
        "indices = np.argsort(feature_importances)[::-1][:20]  # Top 20 features\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"TabNet Feature Importances (Top 20)\")\n",
        "plt.barh(range(len(indices)), feature_importances[indices])\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "for i in range(min(10, len(indices))):\n",
        "    idx = indices[i]\n",
        "    print(f\"{i+1}. {feature_names[idx]}: {feature_importances[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQlZuDacg_Ed"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n📊 {model_name} Evaluation:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC: {auc:.4f}\")\n",
        "    return [acc, prec, rec, f1, auc]\n",
        "\n",
        "results_ann = evaluate_model(model_ann, X_test, y_test, \"ANN\")\n",
        "results_cnn = evaluate_model(model_cnn, X_test_cnn, y_test, \"1D CNN\")\n",
        "results_transformer = evaluate_model(model_transformer, X_test, y_test, \"Transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfgRwRObhOW5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.bar(x - 1.5*width, results_ann, width, label='ANN')\n",
        "ax.bar(x - 0.5*width, results_cnn, width, label='1D CNN')\n",
        "ax.bar(x + 0.5*width, results_transformer, width, label='Transformer')\n",
        "ax.bar(x + 1.5*width, results_tabnet, width, label='TabNet')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison: ANN vs CNN vs Transformer vs TabNet')\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1.1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Metric':<15} {'ANN':<12} {'1D CNN':<12} {'Transformer':<12} {'TabNet':<12}\")\n",
        "print(\"-\"*80)\n",
        "for i, metric in enumerate(labels):\n",
        "    print(f\"{metric:<15} {results_ann[i]:<12.4f} {results_cnn[i]:<12.4f} {results_transformer[i]:<12.4f} {results_tabnet[i]:<12.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaJrbnEx4Cn3"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "9054e5812adb29eebbcd6b680e8ef1afc4fe6e00a75ff130e735bd95b5b32301"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}