{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheJojoJoseph/DL_2025_Group_35/blob/main/churn_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRv8adCyfT6f"
      },
      "source": [
        "# **Telecom Machine Learning Project to Predict Customer Churn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9kuB3TCzJwB",
        "outputId": "d6b5e3fa-1193-4472-ad82-2bddb7af7cd6"
      },
      "outputs": [],
      "source": [
        "# Colab mount only when running in Google Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    pass  # No-op on local environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXZnQ0Q5eo-e"
      },
      "source": [
        "## **Business Overview**\n",
        "\n",
        "The telecommunications industry is a rapidly growing sector that is constantly evolving to meet the demands of consumers. As technology advances and user behavior changes, telecom operators face a variety of challenges that can impact their business success. In order to stay competitive and meet customer needs, it is important for telecom companies to regularly analyze their data to identify relevant problems and opportunities for improvement.\n",
        "\n",
        "This Project is part of the \"telecom data\" cluster of projects, targeting the telecom industry. It is recommended to go through the [\"Exploratory Data Analysis\"](https://www.projectpro.io/data-science-use-cases/telecom-data-analysis-project) notebook before executing this project to understand the Data better.\n",
        "\n",
        "**Aim:**\n",
        "\n",
        "The aim of a churn prediction notebook is to develop a machine learning model that can predict which customers are likely to churn or discontinue their use of a service or product. Churn prediction is a critical business problem for companies that operate on a subscription or recurring revenue model, such as telecommunications companies.\n",
        "\n",
        "While the project will involve building a churn prediction model, the primary focus will be on the importance of monitoring and adapting to changes in the data that may affect the accuracy and effectiveness of the model over time. The project will also emphasize the need for a feedback loop that allows for continuous improvement and refinement of the model based on new data and changing business requirements. By highlighting these concepts, the project aims to help businesses understand the importance of staying agile and adaptable in their machine learning approaches, rather than solely focusing on the accuracy of a single model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![image.png](https://images.unsplash.com/photo-1584438784894-089d6a62b8fa?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yaKuQlUUooQ"
      },
      "source": [
        "\n",
        "\n",
        "**What is Churn Prediction?**\n",
        "\n",
        "Churn prediction is the process of identifying customers who are likely to discontinue using a service or product. In the context of the telecom sector, churn prediction is the process of identifying customers who are likely to switch to a competitor or terminate their contract with their current service provider.\n",
        "\n",
        "Churn prediction is a critical problem in the telecom sector, as it has a significant impact on a service provider's revenue and profitability. The telecom sector is highly competitive, and service providers are constantly vying for customers' attention. It helps service providers identify customers who are at risk of leaving and take proactive measures to retain them.\n",
        "\n",
        "**Challenges**\n",
        "\n",
        "* Churn prediction is a challenging problem, as it involves analyzing large volumes of data from multiple sources. Telecom service providers generate a vast amount of data from customer interactions, network performance, and billing systems. This data is typically stored in disparate systems, making it difficult to analyze and derive insights.\n",
        "\n",
        "* Another challenge in churn prediction is the diversity of customer behavior. Customers have different reasons for leaving a service provider, and these reasons can be difficult to predict. Some customers may leave due to poor network performance, while others may switch to a competitor offering a better deal. Predicting churn accurately requires understanding these different customer behaviors and identifying the most critical predictors of churn.\n",
        "\n",
        "**Business Impact of Churn Prediction**\n",
        "\n",
        "Churn prediction has a significant impact on a telecom service provider's business. A high churn rate can result in a loss of revenue and profitability. On the other hand, an effective churn prediction model can help service providers identify customers who are at risk of leaving and take proactive measures to retain them.\n",
        "\n",
        "Here are some of the key business impacts of churn prediction:\n",
        "\n",
        "* **Revenue Protection**: Churn prediction helps service providers protect their revenue by identifying customers who are likely to leave and taking proactive measures to retain them. This can include offering discounts, upgrading service plans, or providing additional services. By retaining customers, service providers can maintain their revenue stream and avoid the cost of acquiring new customers.\n",
        "\n",
        "* **Customer Retention**: Churn prediction helps service providers retain their existing customers by identifying their needs and preferences. By understanding why customers leave, service providers can make improvements to their service and provide a better customer experience. This can help to build customer loyalty and increase the lifetime value of a customer.\n",
        "\n",
        "* **Cost Reduction**: Churn prediction can help service providers reduce the cost of acquiring new customers. Acquiring new customers is more expensive than retaining existing ones, and churn prediction can help service providers focus their marketing efforts on the most valuable customers.\n",
        "\n",
        "* **Competitive Advantage**: Churn prediction can provide a significant competitive advantage in the telecom sector. By retaining customers and improving the customer experience, service providers can differentiate themselves from their competitors. This can help to increase market share and profitability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdkeckhCKFMo"
      },
      "source": [
        "## **Approach**\n",
        "\n",
        "**Data exploration**\n",
        "\n",
        "* Load the dataset and examine its structure and contents.\n",
        "* Explore the distribution of the target variable (churn) and the features.\n",
        "\n",
        "**Data preprocessing**\n",
        "\n",
        "* Handle missing values by imputing them with appropriate values.\n",
        "* Handle outliers by removing or transforming them.\n",
        "* Encode categorical variables using one-hot encoding.\n",
        "* Scale numerical variables using Standard scaler.\n",
        "\n",
        "\n",
        "**Model training**\n",
        "\n",
        "* Split the data into training and validation sets.\n",
        "* Train logistic regression, random forest, and XGBoost models on the training set.\n",
        "* Evaluate the performance of the models on the validation set using metrics such as accuracy, precision, recall, and F1 score.\n",
        "* Choose the best-performing model based on the evaluation results.\n",
        "\n",
        "**Data drift monitoring**\n",
        "\n",
        "* Use deep checks to monitor for data drift in the input features and the target variable.\n",
        "* Check the model's performance on the validation set regularly to detect any model drift.\n",
        "\n",
        "**Inference pipeline**\n",
        "\n",
        "* Build an inference pipeline to predict churn for new data.\n",
        "* Handle cases where the label (churn) is not present in the input data.\n",
        "* Handle cases where the drift is detected by retraining the model with misclassified data.\n",
        "\n",
        "**Project Summary**\n",
        "\n",
        "* Summarize the results and draw insights from the model's predictions.\n",
        "* Provide recommendations for business actions based on the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0k5VMe9LRWn"
      },
      "source": [
        "## **Learning Outcomes**\n",
        "\n",
        "* Understanding the problem of customer churn and its impact on businesses.\n",
        "\n",
        "* Understanding the importance of data cleaning and preprocessing in building accurate machine learning models.\n",
        "* Learning how to handle missing values, outliers, and categorical variables using one-hot encoding and numerical variables using standard scaler.\n",
        "* Understanding different machine learning algorithms such as logistic regression, random forest, and XGBoost, and their pros and cons in predicting churn.\n",
        "* Learning how to evaluate machine learning models using metrics such as accuracy, precision, recall, and F1 score.\n",
        "* Understanding the importance of monitoring data drift in machine learning models and how to use deep checks to detect it.\n",
        "* Learning how to build an inference pipeline for predicting churn for new data.\n",
        "* Understanding the limitations of the model and the potential impact of false positives and false negatives.\n",
        "* Learning how to provide recommendations for business actions based on the model's predictions, such as targeted marketing campaigns and retention strategies.\n",
        "* Understanding the iterative nature of machine learning and the need for continuous improvement and retraining of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbdDx8JyLweb"
      },
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "* Familiarity with Python programming language and libraries such as NumPy, Pandas, and Scikit-learn.\n",
        "\n",
        "* Basic knowledge of machine learning concepts such as supervised learning, linear regression.\n",
        "\n",
        "* Understanding of data preprocessing techniques such as handling missing values, outliers, and categorical variables.\n",
        "\n",
        "* Knowledge of Jupyter Notebook or any other Python IDE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPncgPDm1_jf"
      },
      "source": [
        "## **Execution Instructions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6fvW78jMQip"
      },
      "source": [
        "# Read data: prefer local CSV if available, else download from Google Drive\n",
        "import os\n",
        "\n",
        "local_paths = [\n",
        "    'Telecom_Data.csv',\n",
        "    './data/Telecom_Data.csv',\n",
        "]\n",
        "\n",
        "# Google Drive direct download link\n",
        "gdrive_file_id = '1Gx5ZGeqB--AtZ3sUS6kZJ0hRm21G1pML'\n",
        "gdrive_url = f'https://drive.google.com/uc?id={gdrive_file_id}'\n",
        "\n",
        "# Check if file exists locally\n",
        "chosen = None\n",
        "for p in local_paths:\n",
        "    if os.path.exists(p):\n",
        "        chosen = p\n",
        "        break\n",
        "\n",
        "# If not found locally, download from Google Drive\n",
        "if chosen is None:\n",
        "    print('Local file not found. Downloading from Google Drive...')\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        print('Installing gdown...')\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', '-q', 'gdown'])\n",
        "        import gdown\n",
        "    \n",
        "    # Download the file\n",
        "    output_path = 'Telecom_Data.csv'\n",
        "    gdown.download(gdrive_url, output_path, quiet=False)\n",
        "    chosen = output_path\n",
        "    print(f'Download complete: {chosen}')\n",
        "else:\n",
        "    print(f'Using local file: {chosen}')\n",
        "\n",
        "print(f'Reading data from: {chosen}')\n",
        "df = pd.read_csv(chosen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB7mcZqH9Hsf"
      },
      "source": [
        "## **Important Libraries**\n",
        "\n",
        "* **pandas**: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool built on top of the Python programming language. Refer to [documentation](https://pandas.pydata.org/) for more information.\n",
        "\n",
        "* **NumPy**: The fundamental package for scientific computing with Python. Fast and versatile, the NumPy vectorization, indexing, and broadcasting concepts are the de-facto standards of array computing today. NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more. Refer to [documentation](https://numpy.org/) for more information. pandas and NumPy are together used for most of the data analysis and manipulation in Python.\n",
        "\n",
        "* **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to [documentation](https://matplotlib.org/) for more information.\n",
        "\n",
        "* **seaborn**: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Refer to [documentation](https://seaborn.pydata.org/) for more information.\n",
        "\n",
        "* **scikit-learn**: Simple and efficient tools for predictive data analysis\n",
        "accessible to everybody and reusable in various contexts.\n",
        "It is built on NumPy, SciPy, and matplotlib to support machine learning in Python. Refer to [documentation](https://scikit-learn.org/stable/) for more information.\n",
        "\n",
        "* **Warnings**:The warnings library provides a way to handle warnings that are generated during program execution. Warnings are typically issued when there is a potential issue with code, but the code still runs without errors. The warnings module provides a way to catch these warnings and handle them in a way that is appropriate for the program. This can be especially useful when developing and debugging code, as warnings can help identify potential issues before they become errors.\n",
        "\n",
        "* **sys**:The sys library provides access to some system-specific parameters and functions. This library can be used to access system-level information, such as the command line arguments passed to the program, the version of the Python interpreter being used, and more.\n",
        "\n",
        "* **xgboost**: xgboost is an open-source machine learning library designed to be highly efficient, scalable, and portable. It is a gradient boosting algorithm that is used for supervised learning problems, including regression, classification, and ranking. Refer to [documentation](https://xgboost.readthedocs.io/en/stable/install.html) for more information.\n",
        "\n",
        "* **deepchecks**: deepchecks is an open-source library that is used for deep learning model debugging and monitoring. It provides a suite of tools for detecting and diagnosing common errors that can occur during the training and evaluation of deep learning models. Refer to [documentation](https://docs.deepchecks.com/stable/getting-started/welcome.html) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdQ82V3M_-R"
      },
      "source": [
        "## **Package Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2KU4tEjC15Jr"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8qAwTZ6tM-5k"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler, OneHotEncoder\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlp82fzIY6xx"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Dj_tQGTTkB"
      },
      "source": [
        "After package installation, kindly restart the runtime if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbG5fh1i9Q3U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-I2Xd4o4uKB"
      },
      "source": [
        "## **Data Reading from Different Sources**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ffu_ikbNanX"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here to view more</summary>\n",
        "\n",
        "\n",
        "### **Files**\n",
        "\n",
        "In many cases, the data is stored in the local system. To read the data from the local system, specify the correct path and filename.\n",
        "\n",
        "### **CSV Format**\n",
        "Comma-separated values, also known as CSV, is a specific way to store data in a table structure format. The data used in this project is stored in a CSV file. Download the data for the project.\n",
        "\n",
        "\n",
        "Use following code to read data from csv file using pandas.\n",
        "```\n",
        "import pandas as pd\n",
        "csv_file_path= \"D:/ProjectPro/Telecom Machine Learning Project to Predict Customer Churn/data/Telecom_data.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "```\n",
        "With appropriate csv_file_path, pd.read_csv() function will read the data and store it in df variable.\n",
        "\n",
        "If you get *FileNotFoundError or No such file or directory*, try checking the path provided in the function. It's possible that python is not able to find the file or directory at a given location.\n",
        "\n",
        "\n",
        "### **Colab - CSV Format**\n",
        "\n",
        "```\n",
        "# mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "csv_file_path= '/content/drive/MyDrive/project_pro/Telecom Machine Learning Project to Predict Customer Churn/Telecom_data.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "```\n",
        "\n",
        "### **AWS S3 - CSV**\n",
        "Use the S3 public link to read the CSV file directly into a pandas DataFrame\n",
        "```\n",
        "s3_link = 'https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv'\n",
        "df = pd.read_csv(s3_link)\n",
        "```\n",
        "\n",
        "### **Database**\n",
        "Most organizations store their data in databases such as MS SQL. Microsoft SQL Server (MS SQL) is a relational database management system developed by Microsoft. A BAK file in Microsoft SQL Server is a backup file that contains a copy of a SQL Server database at a specific point in time. It is essentially a binary representation of the database and includes all its data, tables, schema, indexes, stored procedures, and other objects.\n",
        "\n",
        "#### **Installing MS SQL Management Studio**\n",
        "To install Microsoft SQL Server Management Studio, you can follow these steps:\n",
        "\n",
        "* Go to the Microsoft SQL Server Downloads page (https://www.microsoft.com/en-us/sql-server/sql-server-downloads) and click on the \"Download now\" button for the version of SQL Server Management Studio that you want to install.\n",
        "* Follow the instructions on the screen to download the installation file to your computer.\n",
        "* Once the download is complete, locate the installation file and double-click on it to start the installation process.\n",
        "\n",
        "\n",
        "#### **Restore a BAK file in MS SQL**\n",
        "\n",
        "* Open SQL Server Management Studio and connect to the SQL Server instance to which you want to upload the BAK file.\n",
        "* Right-click on the Databases folder in the Object Explorer pane and select \"Restore Database...\" from the context menu.\n",
        "* In the \"Restore Database\" dialog box, select the \"Device\" option under the \"Source\" section.\n",
        "* Click on the \"...\" button to open the \"Select backup devices\" dialog box.\n",
        "In the \"Select backup devices\" dialog box, click on the \"Add\" button to add the BAK file that you want to upload.\n",
        "* In the \"Locate Backup File\" dialog box, browse to the location where the BAK file is stored in the Telecom Data Analysis Project to Improve Service Quality directory under the ‘data’ folder, select the file, and click on the \"OK\" button.\n",
        "* Back in the \"Select backup devices\" dialog box, the BAK file you added should now be listed under \"Backup media:\".\n",
        "* Click on the \"OK\" button to close the \"Select backup devices\" dialog box.\n",
        "In the \"Restore Database\" dialog box, you should see the BAK file listed in the \"Backup sets to restore\" section.\n",
        "* By default, the original database name and file locations from the BAK file will be used. If you want to restore the database with a different name or to a different location, you can modify the \"To database\" and \"Restore as\" options under the \"General\" section.\n",
        "* Click the \"Options\" tab for additional restore options.\n",
        "* If you want to overwrite an existing database with the same name, you can select the \"Overwrite the existing database (WITH REPLACE)\" option under the \"Restore options\" section.\n",
        "* Click on the \"OK\" button to start the restore process.\n",
        "* Once the restore process is complete, you should see a message indicating that the restore was successful.\n",
        "\n",
        "#### **Read Data from DB to Python**\n",
        " The data can be accessed by secret credentials, which will be in the following format.\n",
        "```\n",
        "import pyodbc\n",
        "import pandas as pd\n",
        "connection = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};\\\n",
        "                       SERVER=server_name;\\\n",
        "                       PORT=1433;\\\n",
        "                       DATABASE=database_name;\\\n",
        "                       UID=admin;\\\n",
        "                       PWD=password')\n",
        "```\n",
        "#### **Steps to install ODBC driver**\n",
        "* Go to the Microsoft Download Center page for the ODBC Driver 17 for SQL Server: https://www.microsoft.com/en-us/download/details.aspx?id=56567\n",
        "* Select the download button that corresponds to the operating system you are using.\n",
        "* Select the language you want to use for the installer, then click the download button.\n",
        "* Once the download is complete, run the installer.\n",
        "* Accept the license terms, then select the features you want to install.\n",
        "Choose a location to install the driver, or use the default location.\n",
        "* Complete the installation process by following the instructions provided by the installer.\n",
        "* Once the installation is complete, you can use the ODBC Driver 17 for SQL Server to connect to SQL Server databases from applications that support ODBC connectivity.\n",
        "\n",
        "#### **Query to read the data into Pandas**\n",
        "\n",
        "```\n",
        "\n",
        "query = '''select * from Processed_month_1_data\n",
        "           UNION ALL\n",
        "            select * from Processed_month_2_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_3_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_4_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_5_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_6_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_7_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_8_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_9_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_10_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_11_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_12_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_13_data\n",
        "            UNION ALL\n",
        "            select * from Processed_month_14_data'''\n",
        "processed_data = pd.read_sql(query,connection)\n",
        "processed_data.head()\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cGA-Er4Z-2i"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WNyymnhZ-2i"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-izhMtyrZ-2j"
      },
      "outputs": [],
      "source": [
        "# Read data: prefer local CSV if available, else use S3 public link\n",
        "import os\n",
        "local_paths = [\n",
        "    'Telecom_Data.csv',\n",
        "    './data/Telecom_Data.csv',\n",
        "]\n",
        "s3_link = 'https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv'\n",
        "chosen = None\n",
        "for p in local_paths:\n",
        "    if os.path.exists(p):\n",
        "        chosen = p\n",
        "        break\n",
        "if chosen is None:\n",
        "    chosen = s3_link\n",
        "print(f'Reading data from: {chosen}')\n",
        "df = pd.read_csv(chosen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBC_yfBlN3mc"
      },
      "source": [
        "## **Data Exploration**\n",
        "\n",
        "Data exploration is a critical step in the data analysis process, where you examine the dataset to gain a preliminary understanding of the data, detect patterns, and identify potential issues that may need further investigation. Data exploration is important because it helps to provide a solid foundation for subsequent data analysis tasks, hypothesis testing and data visualization.\n",
        "\n",
        "Data exploration is also important because it can help you to identify an appropriate approach for analyzing the data.\n",
        "\n",
        "Here are the various functions that help us explore and understand the data.\n",
        "\n",
        "* Shape: Shape is used to identify the dimensions of the dataset. It gives the number of rows and columns present in the dataset. Knowing the dimensions of the dataset is important to understand the amount of data available for analysis and to determine the feasibility of different methods of analysis.\n",
        "\n",
        "* Head: The head function is used to display the top five rows of the dataset. It helps us to understand the structure and organization of the dataset. This function gives an idea of what data is present in the dataset, what the column headers are, and how the data is organized.\n",
        "\n",
        "* Tail: The tail function is used to display the bottom five rows of the dataset. It provides the same information as the head function but for the bottom rows. The tail function is particularly useful when dealing with large datasets, as it can be time-consuming to scroll through all the rows.\n",
        "\n",
        "* Describe: The describe function provides a summary of the numerical columns in the dataset. It includes the count, mean, standard deviation, minimum, and maximum values, as well as the quartiles. It helps to understand the distribution of the data, the presence of any outliers, and potential issues that can affect the model's accuracy.\n",
        "\n",
        "* Isnull: The isnull function is used to identify missing values in the dataset. It returns a Boolean value for each cell, indicating whether it is null or not. This function is useful to identify the presence of missing data, which can be problematic for regression analysis.\n",
        "\n",
        "* Dropna: The dropna function is used to remove rows or columns with missing data. It is used to remove any observations or variables with missing data, which can lead to biased results in the regression analysis. The dropna function is used after identifying the missing data with the isnull function.\n",
        "\n",
        "* Columns: The .columns method is a built-in function that is used to display the column names of a pandas DataFrame or Series. It returns an array-like object that contains the names of the columns in the order in which they appear in the original DataFrame or Series. It can be used to obtain a quick overview of the variables in a dataset and their names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fqx0MsIZ-2j",
        "outputId": "46819b2d-9ae6-4c71-c096-8b178c03d423"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the Dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMm321OAZ-2k",
        "outputId": "b324b414-eb0c-4bf1-8f41-5b44eb64be9a"
      },
      "outputs": [],
      "source": [
        "# Check the Information of the Dataframe, datatypes and non-null counts\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyb0dMULZ-2k",
        "outputId": "0827cbd7-f21e-46ca-a6db-004b78da68d3"
      },
      "outputs": [],
      "source": [
        "# Checking the names of the columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXTBcfTrORWl"
      },
      "source": [
        "## **Data Dictionary**\n",
        "\n",
        "\n",
        "\n",
        "| Column name\t | Description|\n",
        "| ----- | ----- |\n",
        "| Customer ID\t | Unique identifier for each customer |\n",
        "| Month | Calendar Month- 1:12 |\n",
        "| Month of Joining |\tCalender Month -1:14, Month for which the data is captured|\n",
        "| zip_code |\tZip Code|\n",
        "|Gender |\tGender|\n",
        "| Age |\tAge(Years)|\n",
        "| Married |\tMarital Status |\n",
        "|Dependents | Dependents - Binary |\n",
        "| Number of Dependents |\tNumber of Dependents|\n",
        "|Location ID |\tLocation ID|\n",
        "|Service ID\t |Service ID|\n",
        "|state|\tState|\n",
        "|county\t|County|\n",
        "|timezone\t|Timezone|\n",
        "|area_codes|\tArea Code|\n",
        "|country\t|Country|\n",
        "|latitude|\tLatitude|\n",
        "|longitude\t|Longitude|\n",
        "|arpu|\tAverage revenue per user|\n",
        "|roam_ic\t|Roaming incoming calls in minutes|\n",
        "|roam_og\t|Roaming outgoing calls in minutes|\n",
        "|loc_og_t2t|\tLocal outgoing calls within same network in minutes|\n",
        "|loc_og_t2m\t|Local outgoing calls outside network in minutes(outside same + partner network)|\n",
        "|loc_og_t2f|\tLocal outgoing calls with Partner network in minutes|\n",
        "|loc_og_t2c\t|Local outgoing calls with Call Center in minutes|\n",
        "|std_og_t2t|\tSTD outgoing calls within same network in minutes|\n",
        "|std_og_t2m|\tSTD outgoing calls outside network in minutes(outside same + partner network)|\n",
        "|std_og_t2f|\tSTD outgoing calls with Partner network in minutes|\n",
        "|std_og_t2c\t|STD outgoing calls with Call Center in minutes|\n",
        "|isd_og|\tISD Outgoing calls|\n",
        "|spl_og\t|Special Outgoing calls|\n",
        "|og_others|\tOther Outgoing Calls|\n",
        "|loc_ic_t2t|\tLocal incoming calls within same network in minutes|\n",
        "|loc_ic_t2m|\tLocal incoming calls outside network in minutes(outside same + partner network)|\n",
        "|loc_ic_t2f\t|Local incoming calls with Partner network in minutes|\n",
        "|std_ic_t2t\t|STD incoming calls within same network in minutes|\n",
        "|std_ic_t2m\t|STD incoming calls outside network in minutes(outside same + partner network)|\n",
        "|std_ic_t2f|\tSTD incoming calls with Partner network in minutes|\n",
        "|std_ic_t2o|\tSTD incoming calls operators other networks in minutes|\n",
        "|spl_ic|\tSpecial Incoming calls in minutes|\n",
        "|isd_ic|\tISD Incoming calls in minutes|\n",
        "|ic_others|\tOther Incoming Calls|\n",
        "|total_rech_amt|\tTotal Recharge Amount in Local Currency|\n",
        "|total_rech_data|\tTotal Recharge Amount for Data in Local Currency\n",
        "|vol_4g|\t4G Internet Used in GB|\n",
        "|vol_5g|\t5G Internet used in GB|\n",
        "|arpu_5g|\tAverage revenue per user over 5G network|\n",
        "|arpu_4g|\tAverage revenue per user over 4G network|\n",
        "|night_pck_user|\tIs Night Pack User(Specific Scheme)|\n",
        "|fb_user|\tSocial Networking scheme|\n",
        "|aug_vbc_5g|\tVolume Based cost for 5G network (outside the scheme paid based on extra usage)|\n",
        "|offer|\tOffer Given to User|\n",
        "|Referred a Friend|\tReferred a Friend : Binary|\n",
        "|Number of Referrals|\tNumber of Referrals|\n",
        "|Phone Service|\tPhone Service: Binary|\n",
        "|Multiple Lines|\tMultiple Lines for phone service: Binary|\n",
        "|Internet Service|\tInternet Service: Binary|\n",
        "|Internet Type|\tInternet Type|\n",
        "|Streaming Data Consumption|\tStreaming Data Consumption|\n",
        "|Online Security|\tOnline Security|\n",
        "|Online Backup|\tOnline Backup|\n",
        "|Device Protection Plan|\tDevice Protection Plan|\n",
        "|Premium Tech Support|\tPremium Tech Support|\n",
        "|Streaming TV|\tStreaming TV|\n",
        "|Streaming Movies|\tStreaming Movies|\n",
        "|Streaming Music|\tStreaming Music|\n",
        "|Unlimited Data|\tUnlimited Data|\n",
        "|Payment Method|\tPayment Method|\n",
        "|Status ID|\tStatus ID|\n",
        "|Satisfaction Score|\tSatisfaction Score|\n",
        "|Churn Category|\tChurn Category|\n",
        "|Churn Reason|\tChurn Reason|\n",
        "|Customer Status|\tCustomer Status|\n",
        "|Churn Value|\tBinary Churn Value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "CYI9w9ZQZ-2k",
        "outputId": "92d6d75f-4519-487a-8c35-8c4a7bec1f48"
      },
      "outputs": [],
      "source": [
        "# Null values sum\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P-B9TxjZ-2l",
        "outputId": "8408d5b2-8c75-4a2f-a900-9dcf71ef2c7d"
      },
      "outputs": [],
      "source": [
        "# Null values in total recharge data\n",
        "df['total_rech_data'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFPE07KUZ-2l",
        "outputId": "90e7875a-b125-4e91-86d8-b9466adf46c7"
      },
      "outputs": [],
      "source": [
        "# Null values in Internet Type\n",
        "df['Internet Type'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20Xrd0RHZ-2l",
        "outputId": "2f7d8db2-3235-4412-f2d3-a81c2a0ad8a9"
      },
      "outputs": [],
      "source": [
        "# Missing value percentage\n",
        "df['total_rech_data'].isna().sum()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmWVmQQXZ-2l"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*  These missing values may represent customers who have not recharged their account or have recharged but the information has not been recorded.\n",
        "\n",
        "* It is possible that customers with missing recharge data are those who received free data service, and therefore did not need to recharge their account. Alternatively, it is possible that the missing values are due to technical issues, such as data recording errors or system failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "vvV6-BuZZ-2m",
        "outputId": "1dd75cc8-6800-4a20-bc02-295f41895508"
      },
      "outputs": [],
      "source": [
        "# Checking the value counts of Internet Service where total recharge data was null\n",
        "df[df['total_rech_data'].isna()]['Internet Service'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR2RpuGKZ-2m"
      },
      "source": [
        "**Observation**:\n",
        "\n",
        "* It turns out that all customers with missing recharge data have opted for internet service, the next step could be to check if they have used it or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "PBgsjUoPZ-2m",
        "outputId": "b8543893-c7bf-4e38-ee14-9e3854a9ab89"
      },
      "outputs": [],
      "source": [
        "# Let's check unlimited data column\n",
        "df[(df['total_rech_data'].isna())]['Unlimited Data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "fVj27H0xZ-2m",
        "outputId": "8f747bda-b50e-4bec-eefb-3d99c033cf67"
      },
      "outputs": [],
      "source": [
        "# Lets check Average Revenue for 4g and 5g\n",
        "df[(df['total_rech_data'].isna())][['arpu_4g','arpu_5g']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z--ECzlXZ-2m"
      },
      "source": [
        "**Observation**:\n",
        "\n",
        "* We can fill the missing values in the total_rech_data column with 0 when the arpu (Average Revenue Per User) is not applicable. This is because the arpu is a measure of the revenue generated per user, and if it is not applicable, it may indicate that the user is not generating any revenue for the company. In such cases, it is reasonable to assume that the total recharge amount is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "HKsHAQQcZ-2n",
        "outputId": "05485aa1-0b2e-4be5-f478-4f2a87ae659c"
      },
      "outputs": [],
      "source": [
        "# Check the value counts of ARPU 4g and 5g\n",
        "df[['arpu_4g','arpu_5g']].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHibU4lcZ-2n"
      },
      "outputs": [],
      "source": [
        "# Replacing all values of total recharge data= 0 where arpu 4g and 5g are not applicable\n",
        "df.loc[(df['arpu_4g']=='Not Applicable') | (df['arpu_5g']=='Not Applicable'),'total_rech_data']=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZywhbWfZ-2n",
        "outputId": "218acee6-5921-41a4-a3ff-b269e00d3efc"
      },
      "outputs": [],
      "source": [
        "# Missing value percentage\n",
        "df['total_rech_data'].isna().sum()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvS-GOYdZ-2n"
      },
      "source": [
        "We cannot fill other values with 0 because they have some ARPU to consider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xivTAuUZ-2n",
        "outputId": "9ed5a169-2c13-46c9-b1c0-4a5960b1c60e"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n",
        "df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5NNUYX8Z-2n"
      },
      "source": [
        "With this mean, we will fill the NaN values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePxg16ZkZ-2n"
      },
      "outputs": [],
      "source": [
        "# Fill NaN values in 'total_rech_data' with the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n",
        "df['total_rech_data']=df['total_rech_data'].fillna(df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "2y8-hQOgZ-2n",
        "outputId": "4d4bbc1d-6d81-437d-d66c-c7510874a2bf"
      },
      "outputs": [],
      "source": [
        "# Check the value counts for Internet Type\n",
        "df['Internet Type'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "VojpiRlLZ-2o",
        "outputId": "edf589c7-dfa9-4830-da0b-0613b7d1bffb"
      },
      "outputs": [],
      "source": [
        "# Check value counts for Internet Service where Internet Type is null\n",
        "df[df['Internet Type'].isna()]['Internet Service'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDvmPBoeZ-2o"
      },
      "source": [
        "All null values in Internet Type does not have Internet Service. Let's fill these null values with Not Applicable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjmE5bcbZ-2o"
      },
      "outputs": [],
      "source": [
        "# Filling Null values in Internet Type\n",
        "df['Internet Type']=df['Internet Type'].fillna('Not Applicable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoezoPLWZ-2o",
        "outputId": "e04fc316-bfa5-4661-96c5-950bee5bd83d"
      },
      "outputs": [],
      "source": [
        "# Shape of the dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjO5_qGIZ-2o"
      },
      "outputs": [],
      "source": [
        "# Insert a new column named 'total_recharge' before the last column in the dataframe\n",
        "# The values of 'total_recharge' are the sum of 'total_rech_amt' and 'total_rech_data'\n",
        "df.insert(loc=df.shape[1]-1,column='total_recharge',value=df['total_rech_amt']+df['total_rech_data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "wkQYshrQZ-2o",
        "outputId": "4ec8040f-69ee-4617-b24d-a548debcd431"
      },
      "outputs": [],
      "source": [
        "# Checking percent of missing values in columns\n",
        "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
        "df_missing_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHK6wY6OZ-2o"
      },
      "source": [
        "Let's drop some unnecessary columns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRayCzDDZ-2o"
      },
      "outputs": [],
      "source": [
        "# Dropping columns\n",
        "df=df.drop(columns=['night_pck_user', 'fb_user','Churn Category','Churn Reason', 'Customer Status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCVlehSCZ-2o",
        "outputId": "ede0e576-2a75-4de1-fb10-2bccfc8c8fef"
      },
      "outputs": [],
      "source": [
        "# Checking churn %\n",
        "round(100*(df['Churn Value'].mean()),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d99fGw_Z-2p",
        "outputId": "6ec27f10-9990-4e06-a5c6-9e1d11806ac9"
      },
      "outputs": [],
      "source": [
        "# Number of unique latitudes\n",
        "df['latitude'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cHkBum8Z-2p",
        "outputId": "6b2aba0b-103e-4a60-cd9d-fdc01c4ee923"
      },
      "outputs": [],
      "source": [
        "# Number of unique longitudes\n",
        "df['longitude'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mp5w1niZ-2p"
      },
      "source": [
        "Replace 'Not Applicable' with 0 in both 'arpu_4g' and 'arpu_5g'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74bP-j3tZ-2p"
      },
      "outputs": [],
      "source": [
        "# Replace 'Not Applicable' with 0 in 'arpu_4g'\n",
        "df['arpu_4g'] = df['arpu_4g'].replace('Not Applicable', 0)\n",
        "\n",
        "# Replace 'Not Applicable' with 0 in 'arpu_5g'\n",
        "df['arpu_5g'] = df['arpu_5g'].replace('Not Applicable', 0)\n",
        "\n",
        "# Convert 'arpu_4g' to float data type\n",
        "df['arpu_4g'] = df['arpu_4g'].astype(float)\n",
        "\n",
        "# Convert 'arpu_5g' to float data type\n",
        "df['arpu_5g'] = df['arpu_5g'].astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "NypxsZ4JZ-2p",
        "outputId": "bb8c094b-83d4-4805-fb20-47a2c49e357e"
      },
      "outputs": [],
      "source": [
        "# Check the data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HS3WicKZ-2p"
      },
      "outputs": [],
      "source": [
        "# Note: We are keeping customer location-based attributes aside for now\n",
        "location_att=['zip_code''state', 'county', 'timezone', 'area_codes', 'country','latitude','longitude']\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_cols=['Gender',\n",
        "       'Married', 'Dependents',\n",
        "       'offer','Referred a Friend', 'Phone Service',\n",
        "       'Multiple Lines', 'Internet Service', 'Internet Type',\n",
        "        'Online Security', 'Online Backup',\n",
        "       'Device Protection Plan', 'Premium Tech Support', 'Streaming TV',\n",
        "       'Streaming Movies', 'Streaming Music', 'Unlimited Data',\n",
        "       'Payment Method']\n",
        "\n",
        "# List of continuous columns\n",
        "cts_cols=['Age','Number of Dependents',\n",
        "       'roam_ic', 'roam_og', 'loc_og_t2t',\n",
        "       'loc_og_t2m', 'loc_og_t2f', 'loc_og_t2c', 'std_og_t2t', 'std_og_t2m',\n",
        "       'std_og_t2f', 'std_og_t2c', 'isd_og', 'spl_og', 'og_others',\n",
        "       'loc_ic_t2t', 'loc_ic_t2m', 'loc_ic_t2f', 'std_ic_t2t', 'std_ic_t2m',\n",
        "       'std_ic_t2f', 'std_ic_t2o', 'spl_ic', 'isd_ic', 'ic_others',\n",
        "       'total_rech_amt', 'total_rech_data', 'vol_4g', 'vol_5g', 'arpu_5g',\n",
        "       'arpu_4g', 'arpu', 'aug_vbc_5g', 'Number of Referrals','Satisfaction Score',\n",
        "       'Streaming Data Consumption']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfLU8h3Z15Jz"
      },
      "source": [
        "### **Outlier Detection**:\n",
        "\n",
        "Outlier detection is a critical data analysis technique that involves identifying and removing data points that are significantly different from the rest of the data. Outliers are data points that lie far away from the rest of the data, and they can significantly influence the statistical analysis and machine learning models' performance. Therefore, identifying and removing outliers is essential to ensure accurate and reliable data analysis results.\n",
        "\n",
        "There are two main approaches for outlier detection: parametric and non-parametric.\n",
        "\n",
        "* Parametric Methods:\n",
        "Parametric methods assume that the data follows a specific distribution, such as a normal distribution. In this approach, outliers are identified by calculating the distance of each data point from the mean of the distribution in terms of the number of standard deviations. Data points that are beyond a certain number of standard deviations (usually three or more) are considered as outliers.\n",
        "\n",
        "One common parametric method is the Z-score method, which calculates the distance of each data point from the mean in terms of standard deviations.\n",
        "Parametric methods can be useful when the data follows a known distribution, but they may not be effective when the data is not normally distributed.\n",
        "\n",
        "* Non-Parametric Methods:\n",
        "Non-parametric methods do not assume any specific distribution of the data. Instead, they rely on the rank or order of the data points. In this approach, outliers are identified by comparing the values of each data point with the values of other data points. Data points that are significantly different from other data points are considered as outliers.\n",
        "\n",
        "Quantiles are an important concept in non-parametric outlier detection methods. They represent values that divide a dataset into equal-sized parts, such as quarters or thirds. The most commonly used quantiles are the median (which divides the data into two equal parts), the first quartile (which divides the data into the lowest 25% and the highest 75%), and the third quartile (which divides the data into the lowest 75% and the highest 25%).\n",
        "\n",
        "The interquartile range (IQR) is another important concept related to quantiles. It is defined as the difference between the third and first quartiles and represents the middle 50% of the data. The IQR can be used to identify outliers by defining a range (known as the Tukey's fence) beyond which any data points are considered outliers.\n",
        "Non-parametric methods can be useful when the data is not normally distributed or when the distribution is unknown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqaJC1SzZ-2p"
      },
      "outputs": [],
      "source": [
        "# Create an empty dataframe with columns as cts_cols and index as quantiles\n",
        "quantile_df=pd.DataFrame(columns=cts_cols,index=[0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])\n",
        "\n",
        "# for each column in cts_cols, calculate the corresponding quantiles and store them in the quantile_df\n",
        "for col in cts_cols:\n",
        "   quantile_df[col]=df[col].quantile([0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRSmmv015Jz"
      },
      "source": [
        "By calculating quantiles for each continuous variable in the dataset, we are trying to get an idea about the spread and distribution of the data. Specifically, we are interested in identifying potential outliers in the data.\n",
        "\n",
        "Quantiles divide a distribution into equal proportions. For instance, the 0.25 quantile is the value below which 25% of the observations fall and the 0.75 quantile is the value below which 75% of the observations fall. By calculating quantiles at various levels, we can get a better understanding of the distribution of the data and identify any observations that are too far away from the rest of the data.\n",
        "\n",
        "These quantiles can be used as thresholds to identify potential outliers in the data. Observations with values beyond these thresholds can be considered as potential outliers and further investigation can be carried out to determine if they are true outliers or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "fH6bWCegZ-2p",
        "outputId": "22c3de98-be99-40b0-b7c4-70504f2b5eb4"
      },
      "outputs": [],
      "source": [
        "# Let's check out the quantiles df\n",
        "quantile_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqnLtZs1Z-2p"
      },
      "source": [
        "Outliers were detected in the variables vol_5g, arpu_4g, and arpu_5g."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "U6NQZsd3Z-2q",
        "outputId": "43b756cf-6202-454a-e32e-6bf182190612"
      },
      "outputs": [],
      "source": [
        "# Checking further\n",
        "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcx5-kVNZ-2q",
        "outputId": "cee9ac16-0ab3-4d67-f026-04af34b42d18"
      },
      "outputs": [],
      "source": [
        "# Calculate the proportion of rows in the DataFrame where the value in the 'arpu_4g' column is equal to 254687\n",
        "df[df['arpu_4g']==254687].shape[0]/df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "YJjw0lS5Z-2q",
        "outputId": "5d7f5820-4544-4a63-832f-dfd8e7b660f1"
      },
      "outputs": [],
      "source": [
        "# Let's check it out\n",
        "df[df['arpu_4g']==254687]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzuy2bvZ-2q"
      },
      "source": [
        "Let's see what is the value of 'total_rech_data' for these observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "GJ8kQmwSZ-2q",
        "outputId": "07f0dd03-d0d7-4f9a-f3be-cee72a23ce9c"
      },
      "outputs": [],
      "source": [
        "# Get the value counts of 'total_rech_data' for observations where the value in the 'arpu_4g' column is equal to 254687\n",
        "df[df['arpu_4g']==254687]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4c454tIZ-2q"
      },
      "source": [
        "Now, since the recharge amount is 0 and there is no ARPU, let's replace it with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akCqNl15Z-2q"
      },
      "outputs": [],
      "source": [
        "# Replace the outlier value 254687 in the 'arpu_4g' column of the dataframe 'df' with 0.\n",
        "df['arpu_4g']=df['arpu_4g'].replace(254687,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "s2m3EgcRZ-2q",
        "outputId": "61161ef7-cdbf-4140-eace-3b7dcf5dd08f"
      },
      "outputs": [],
      "source": [
        "# Checking further\n",
        "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "Crbcd8PaZ-2q",
        "outputId": "f163832d-c773-4b0d-96d5-b420b0fb6c53"
      },
      "outputs": [],
      "source": [
        "# Filter by 'arpu_4g' value of 87978 and count unique values in 'total_rech_data' column\n",
        "df[df['arpu_4g']==87978]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jocWPh15J1"
      },
      "source": [
        "All rows in the dataframe with an 'arpu_4g' value of 87978 have 0 value in the 'total_rech_data' column, indicating that these are likely outliers. Therefore, we have decided to replace the 'arpu_4g' value for these rows with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuOjiovGZ-2q"
      },
      "outputs": [],
      "source": [
        "# Replace the values with 0\n",
        "df['arpu_4g']=df['arpu_4g'].replace(87978,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "I7aizZWaZ-2q",
        "outputId": "cec0fe36-5ccf-4c39-c112-8122346cf919"
      },
      "outputs": [],
      "source": [
        "# Checking the quantiles again\n",
        "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "dJp0H0BRZ-2r",
        "outputId": "b8949c1d-3ec5-4bfc-a74f-497262396d88"
      },
      "outputs": [],
      "source": [
        "# Check the churn value for this ARPU\n",
        "df[df['arpu_4g']>8000]['Churn Value'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrZP-iq5Z-2r"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        " * A higher ARPU suggests that a business is generating more revenue per user, which can be a positive sign for the business's profitability. However, a high ARPU can also imply churn, or the rate at which customers are leaving the business.\n",
        "\n",
        "* There are a few reasons why a high ARPU may imply churn. First, if a business is charging a high price for its services, it may attract a customer base that is more price-sensitive and likely to switch to a competitor if they find a better deal. This could result in a higher churn rate for the business."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "bwB9svs2Z-2r",
        "outputId": "8d567b1a-3f76-4b28-e66c-65837f2eabfa"
      },
      "outputs": [],
      "source": [
        "# Check the value counts of total recharge data at outlying values\n",
        "df[df['arpu_5g']==254687]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "HvnOLN9tZ-2r",
        "outputId": "aa1ba7ff-5961-47ce-b5ec-3531439c98f6"
      },
      "outputs": [],
      "source": [
        "# Check the value counts of total recharge data at outlying values\n",
        "df[df['arpu_5g']==87978]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWUCA-0QZ-2r"
      },
      "outputs": [],
      "source": [
        "# Replacing the values with 0 where total recharge data is 0\n",
        "df['arpu_5g']=df['arpu_5g'].replace([87978,254687],0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "eZ2G0_CgZ-2r",
        "outputId": "e5d5ebd7-9009-45f0-fb43-f03f99e35cb6"
      },
      "outputs": [],
      "source": [
        "# Check the quantiles of ARPU 5G\n",
        "df['arpu_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "288nXt4mZ-2r",
        "outputId": "76d36717-aed1-4d08-b3bf-8c80a77689a8"
      },
      "outputs": [],
      "source": [
        "# Check the quantiles of Volume of 5G data\n",
        "df['vol_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.98,0.99,0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "jk5WDhO4Z-2r",
        "outputId": "c2025da1-83e2-420c-b6b1-378dd4d9cd1b"
      },
      "outputs": [],
      "source": [
        "# Lets see the recharge data value\n",
        "df[df['vol_5g']>=87978]['total_rech_data'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "BnX_gEMTZ-2r",
        "outputId": "7106fb0e-9d25-4545-f99d-4466dc811250"
      },
      "outputs": [],
      "source": [
        "# Proportion of these values\n",
        "df[df['vol_5g']>=87978]['total_rech_data'].value_counts()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GufyTVuZ-2s"
      },
      "source": [
        "**Observation**:\n",
        "\n",
        "There is a presence of 2% outliers in vol 5g, where the values are very high, but their total recharge data is 0. We will fill these outliers with 0, and below are some possible reasons why this could be:\n",
        "\n",
        "* Data recording error: It is possible that there was an error in recording the recharge data for these outliers, leading to an incorrect value of 0. In this case, it would make sense to fill the outliers with 0, as this is likely the correct value.\n",
        "\n",
        "* Promotions or bonuses: Another possibility is that these customers received promotions or bonuses that allowed them to use the service without recharging, leading to a total recharge data of 0. However, these customers may still be using the service heavily, leading to the high values in vol 5g. In this case, filling the outliers with 0 would make sense as it accurately reflects the lack of recharge data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnSUoP5cZ-2s"
      },
      "outputs": [],
      "source": [
        "# Replace the outlier values\n",
        "df['vol_5g']=df['vol_5g'].replace([87978,254687],0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc4VgnXjZ-2s",
        "outputId": "aae9a29a-ae5f-4a21-e2b7-3cba64bf1765"
      },
      "outputs": [],
      "source": [
        "# Unique months\n",
        "df['Month'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyuox--FZ-2s",
        "outputId": "c305070f-a827-4adf-ccd2-93c43b19dd33"
      },
      "outputs": [],
      "source": [
        "# Unique months of joining\n",
        "df['Month of Joining'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiZ5l8yUZ-2s"
      },
      "source": [
        "We will get 4 quarters in month of joining!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-iiHq7o15J3"
      },
      "outputs": [],
      "source": [
        "# # Save Processed data\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/processed_churn_data_1.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCa9t9OFiHtf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VUhYYj7eiMl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQgl0gnaMZz"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/processed_churn_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CY0wR2W15J3"
      },
      "source": [
        "### **Quarterly Churn Analysis**\n",
        "\n",
        "Quarterly churn analysis is a process of analyzing the rate at which customers are leaving a business or discontinuing their services over a period of three months. The analysis is usually done by calculating the churn rate, which is the percentage of customers who have stopped using the service during the quarter.\n",
        "\n",
        "* Timeliness: Quarterly churn analysis and prediction allows for a timely assessment of customer retention and churn rates. By conducting this analysis regularly, businesses can identify any changes in customer behavior and take necessary actions to address them in a timely manner.\n",
        "\n",
        "* Evaluation of strategies: Conducting churn analysis and prediction on a quarterly basis enables businesses to evaluate the effectiveness of their customer retention strategies. If the churn rate has increased, the business can evaluate the strategies implemented in the previous quarter and determine whether they were effective or not. This will allow them to adjust their strategies and improve their customer retention efforts.\n",
        "\n",
        "* Financial impact: Churn has a significant financial impact on businesses. By conducting quarterly churn analysis and prediction, businesses can identify areas where they are losing revenue and take steps to prevent further losses. This will help them to maintain financial stability and growth.\n",
        "\n",
        "* Customer insights: Quarterly churn analysis and prediction can also provide valuable insights into customer behavior and preferences. By analyzing customer behavior and reasons for churn, businesses can identify patterns and trends that will help them to improve their services and retain customers in the future.\n",
        "\n",
        "* Benchmarking: Conducting quarterly churn analysis and prediction allows businesses to benchmark their performance against industry standards and competitors. This will help them to identify areas where they are performing well and areas where they need to improve to stay competitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bixkWnVAZ-2s"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Define a function to map a month to its corresponding quarter\n",
        "def map_month_to_quarter(month):\n",
        "    if math.isnan(month): # Handle NaN values if present\n",
        "        return None\n",
        "    quarter = math.ceil(month / 3)\n",
        "    return quarter\n",
        "\n",
        "# Insert a new column called 'Quarter of Joining' in the DataFrame 'df' and populate it with the quarter corresponding to the 'Month of Joining' column\n",
        "df.insert(loc=1,column='Quarter of Joining',value=df['Month of Joining'].apply(lambda x: map_month_to_quarter(x)))\n",
        "\n",
        "# Insert a new column called 'Quarter' in the DataFrame 'df'and populate it with the quarter corresponding to the 'Month' column\n",
        "df.insert(loc=1,column='Quarter',value= df['Month'].apply(lambda x: map_month_to_quarter(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwJBQcGLZ-2t"
      },
      "outputs": [],
      "source": [
        "# Remove duplicate rows in the DataFrame 'df' based on the 'Customer ID', 'Quarter', and 'Quarter of Joining' columns and keep only the last occurrence of each set of duplicates\n",
        "telco=df.drop_duplicates(subset=['Customer ID','Quarter','Quarter of Joining'],keep='last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VxaOz6retqF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8GvS05j15J3"
      },
      "source": [
        "The 'train_data' DataFrame contains the data of customers who joined in the first quarter and were active in the first quarter. This dataset is used for training the churn prediction model.\n",
        "\n",
        "The 'test_data' DataFrame contains the data of customers who joined in the first quarter and were active in the second quarter. This dataset is used for testing the accuracy of the churn prediction model.\n",
        "\n",
        "The 'prediction_data' DataFrame contains the data of customers who joined in the second quarter and were active in the second quarter. This dataset is used for predicting the churn of customers who joined in the second quarter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO88CQjs15KD",
        "outputId": "7378b996-0e33-4600-d05d-d92b4723c09b"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "I8k_pia3a1KP",
        "outputId": "ea3210fa-ad1d-4713-83c5-d79320c00069"
      },
      "outputs": [],
      "source": [
        "df['Churn Value'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGIy42t3Wxq2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Identify columns to drop from features before scaling\n",
        "# 'Customer ID' is a unique identifier and not a feature for the model.\n",
        "# Other object-type columns like 'state', 'county', 'timezone', 'area_codes', 'country' were noted as 'keeping aside for now'.\n",
        "# 'Month' and 'Month of Joining' are already used to create 'Quarter' features, and can be dropped if not needed for ML directly.\n",
        "# 'zip_code', 'Location ID', 'Service ID', 'Status ID' are numerical identifiers, but not direct features, and can also be dropped.\n",
        "\n",
        "cols_to_drop = [\n",
        "    'Customer ID', 'Month', 'Month of Joining', 'zip_code', 'Location ID', 'Service ID',\n",
        "    'state', 'county', 'timezone', 'area_codes', 'country', 'Status ID'\n",
        "]\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# Drop the target variable 'Churn Value' and the identified columns from X\n",
        "X_df = df.drop(columns=['Churn Value'] + cols_to_drop, errors='ignore')\n",
        "y = df['Churn Value']\n",
        "\n",
        "# List of categorical columns (copied from cell -HS3WicKZ-2p)\n",
        "categorical_cols=['Gender',\n",
        "       'Married', 'Dependents',\n",
        "       'offer','Referred a Friend', 'Phone Service',\n",
        "       'Multiple Lines', 'Internet Service', 'Internet Type',\n",
        "        'Online Security', 'Online Backup',\n",
        "       'Device Protection Plan', 'Premium Tech Support', 'Streaming TV',\n",
        "       'Streaming Movies', 'Streaming Music', 'Unlimited Data',\n",
        "       'Payment Method']\n",
        "\n",
        "\n",
        "# Identify categorical columns that are still of 'object' dtype in X_df\n",
        "# Use the predefined 'categorical_cols' from the notebook and ensure they exist in X_df.\n",
        "categorical_features_for_ohe = [col for col in categorical_cols if col in X_df.columns]\n",
        "\n",
        "# Apply one-hot encoding to the identified categorical features\n",
        "X_processed = pd.get_dummies(X_df, columns=categorical_features_for_ohe, drop_first=True)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpK-XFHZWxuP"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "IWRGtbX4hLRu",
        "outputId": "b1b5db69-b182-4b19-a287-233e4f4b8e4c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model_ann = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history_ann = model_ann.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                            epochs=10, batch_size=32, callbacks=[es])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZVKnTRB2C5t",
        "outputId": "56704cff-fee1-49bd-d44f-3372f99d731d"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "loss, accuracy = model_ann.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR6_pfxFfr-g",
        "outputId": "73a3855f-fb8f-4a4c-f3b5-9f71f8f23e40"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = (model_ann.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "bcZTgorrgo6N",
        "outputId": "de6c628c-4b0a-4440-bec2-1b10c5ad5e51"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "# Reshape input to 3D for Conv1D: (samples, features, 1)\n",
        "X_train_cnn = np.expand_dims(X_train, axis=2)\n",
        "X_test_cnn = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "model_cnn = Sequential([\n",
        "    Conv1D(128, 2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv1D(64, 2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "history_cnn = model_cnn.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test),\n",
        "                            epochs=2, batch_size=32, callbacks=[es])\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "43GQg8pqDcSL",
        "outputId": "107a4f22-160f-45aa-ab3f-96df56bc4415"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def build_transformer(input_shape, num_heads=4, key_dim=32):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "input_shape = X_train.shape[1:]\n",
        "model_transformer = build_transformer(input_shape)\n",
        "\n",
        "# Callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train\n",
        "history_transformer = model_transformer.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "# Save for deployment\n",
        "model_transformer.save(\"transformer_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **TabNet Model (Google AI, 2021)**\n",
        "\n",
        "TabNet is a novel deep learning architecture specifically designed for tabular data. It uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and efficient learning.\n",
        "\n",
        "**Key Features:**\n",
        "- Built-in feature selection via sequential attention\n",
        "- Interpretable feature importance masks\n",
        "- Competitive with gradient boosting methods\n",
        "- Handles both categorical and continuous features natively\n",
        "\n",
        "**Reference:** Arik, S. Ö., & Pfister, T. (2021). TabNet: Attentive Interpretable Tabular Learning. AAAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install pytorch-tabnet if not already installed\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "except ImportError:\n",
        "    import sys\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytorch-tabnet\"])\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and train TabNet model\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "\n",
        "# TabNet requires numpy arrays\n",
        "X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
        "X_test_np = X_test.values if hasattr(X_test, 'values') else X_test\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "y_test_np = y_test.values if hasattr(y_test, 'values') else y_test\n",
        "\n",
        "# Initialize TabNet classifier\n",
        "model_tabnet = TabNetClassifier(\n",
        "    n_d=64,                    # Width of the decision prediction layer\n",
        "    n_a=64,                    # Width of the attention embedding for each mask\n",
        "    n_steps=5,                 # Number of steps in the architecture (decision steps)\n",
        "    gamma=1.5,                 # Coefficient for feature reusage in the masks\n",
        "    n_independent=2,           # Number of independent Gated Linear Units layers at each step\n",
        "    n_shared=2,                # Number of shared Gated Linear Units at each step\n",
        "    lambda_sparse=1e-4,        # Sparsity regularization coefficient\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type='entmax',        # Either 'sparsemax' or 'entmax'\n",
        "    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    seed=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model_tabnet.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_name=['test'],\n",
        "    eval_metric=['auc', 'accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=15,\n",
        "    batch_size=256,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model_tabnet.save_model('tabnet_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate TabNet model\n",
        "y_pred_tabnet = model_tabnet.predict(X_test_np)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "acc_tabnet = accuracy_score(y_test_np, y_pred_tabnet)\n",
        "prec_tabnet = precision_score(y_test_np, y_pred_tabnet)\n",
        "rec_tabnet = recall_score(y_test_np, y_pred_tabnet)\n",
        "f1_tabnet = f1_score(y_test_np, y_pred_tabnet)\n",
        "\n",
        "# Get prediction probabilities for AUC\n",
        "y_pred_proba_tabnet = model_tabnet.predict_proba(X_test_np)[:, 1]\n",
        "auc_tabnet = roc_auc_score(y_test_np, y_pred_proba_tabnet)\n",
        "\n",
        "print(f\"\\n📊 TabNet Evaluation:\")\n",
        "print(f\"Accuracy: {acc_tabnet:.4f}\")\n",
        "print(f\"Precision: {prec_tabnet:.4f}\")\n",
        "print(f\"Recall: {rec_tabnet:.4f}\")\n",
        "print(f\"F1-Score: {f1_tabnet:.4f}\")\n",
        "print(f\"ROC-AUC: {auc_tabnet:.4f}\")\n",
        "\n",
        "results_tabnet = [acc_tabnet, prec_tabnet, rec_tabnet, f1_tabnet, auc_tabnet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize TabNet feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from TabNet\n",
        "feature_importances = model_tabnet.feature_importances_\n",
        "\n",
        "# Get feature names (assuming X_train is a DataFrame, otherwise use indices)\n",
        "if hasattr(X_train, 'columns'):\n",
        "    feature_names = X_train.columns\n",
        "else:\n",
        "    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
        "\n",
        "# Sort features by importance\n",
        "indices = np.argsort(feature_importances)[::-1][:20]  # Top 20 features\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"TabNet Feature Importances (Top 20)\")\n",
        "plt.barh(range(len(indices)), feature_importances[indices])\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "for i in range(min(10, len(indices))):\n",
        "    idx = indices[i]\n",
        "    print(f\"{i+1}. {feature_names[idx]}: {feature_importances[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQlZuDacg_Ed",
        "outputId": "f2d650c3-0a92-4b5f-90b3-1dde901ff7fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n📊 {model_name} Evaluation:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC: {auc:.4f}\")\n",
        "    return [acc, prec, rec, f1, auc]\n",
        "\n",
        "results_ann = evaluate_model(model_ann, X_test, y_test, \"ANN\")\n",
        "results_cnn = evaluate_model(model_cnn, X_test_cnn, y_test, \"1D CNN\")\n",
        "results_transformer = evaluate_model(model_transformer, X_test, y_test, \"Transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "dfgRwRObhOW5",
        "outputId": "8211e914-cbad-416e-bf89-268424344984"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.bar(x - 1.5*width, results_ann, width, label='ANN')\n",
        "ax.bar(x - 0.5*width, results_cnn, width, label='1D CNN')\n",
        "ax.bar(x + 0.5*width, results_transformer, width, label='Transformer')\n",
        "ax.bar(x + 1.5*width, results_tabnet, width, label='TabNet')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison: ANN vs CNN vs Transformer vs TabNet')\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1.1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Metric':<15} {'ANN':<12} {'1D CNN':<12} {'Transformer':<12} {'TabNet':<12}\")\n",
        "print(\"-\"*80)\n",
        "for i, metric in enumerate(labels):\n",
        "    print(f\"{metric:<15} {results_ann[i]:<12.4f} {results_cnn[i]:<12.4f} {results_transformer[i]:<12.4f} {results_tabnet[i]:<12.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **TabNet Model Insights**\n",
        "\n",
        "**Why TabNet for Churn Prediction?**\n",
        "\n",
        "TabNet offers several advantages for this telecom churn prediction task:\n",
        "\n",
        "1. **Interpretability**: Unlike traditional neural networks, TabNet provides feature importance scores that help understand which customer attributes drive churn decisions.\n",
        "\n",
        "2. **Efficient Feature Selection**: The sequential attention mechanism automatically selects relevant features at each decision step, reducing noise from irrelevant features.\n",
        "\n",
        "3. **Tabular Data Optimization**: Specifically designed for tabular data, TabNet often outperforms general-purpose neural networks on structured datasets.\n",
        "\n",
        "4. **No Feature Engineering Required**: TabNet can handle both categorical and numerical features natively without extensive preprocessing.\n",
        "\n",
        "**Key Hyperparameters:**\n",
        "- `n_d` and `n_a`: Control the width of decision and attention layers\n",
        "- `n_steps`: Number of sequential decision steps (higher = more complex reasoning)\n",
        "- `gamma`: Controls feature reusage across steps\n",
        "- `lambda_sparse`: Sparsity regularization for interpretability\n",
        "\n",
        "**Business Value:**\n",
        "The feature importance visualization helps telecom operators understand:\n",
        "- Which customer behaviors most strongly predict churn\n",
        "- Where to focus retention efforts\n",
        "- Which service features matter most to customers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpI2G8IO3-pU"
      },
      "source": [
        "####Summary\n",
        "\n",
        "\n",
        "Certainly! Here's a summary of the code and its output:\n",
        "\n",
        "Data Preprocessing and Exploration:\n",
        "\n",
        "Data Loading: The Telecom_Data.csv file was loaded into a pandas DataFrame.\n",
        "Initial Checks: Displayed the shape, info (data types, non-null counts), and column names of the DataFrame.\n",
        "\n",
        "Missing Values: Identified missing values in total_rech_data and Internet Type.\n",
        "total_rech_data: Initially, about 32% of values were missing. It was observed that these missing values correlated with 'Not Applicable' values in arpu_4g and arpu_5g. These were filled with 0 for 'Not Applicable' ARPU cases, and the mean of total_rech_data for other cases.\n",
        "\n",
        "Internet Type: Missing values were identified where Internet Service was 'No'. These were filled with 'Not Applicable'.\n",
        "\n",
        "Feature Engineering: A new column total_recharge (sum of total_rech_amt and total_rech_data) was created.\n",
        "\n",
        "Column Dropping: night_pck_user, fb_user, Churn Category, Churn Reason, and Customer Status columns were dropped due to high missing values or being redundant.\n",
        "\n",
        "Churn Percentage: Calculated the overall churn rate, which is approximately 4.57%.\n",
        "\n",
        "Outlier Handling: Outliers in arpu_4g, arpu_5g, and vol_5g were identified and replaced with 0, particularly when the total_rech_data for those entries was also 0.\n",
        "\n",
        "Quarterly Features: New columns Quarter and Quarter of Joining were derived from Month and Month of Joining for potential time-series analysis.\n",
        "Feature and Target Separation: Churn Value was separated as the target variable (y), and other relevant columns were used as features (X_df).\n",
        "\n",
        "One-Hot Encoding & Scaling: Categorical features were one-hot encoded (drop_first=True), and numerical features were scaled using StandardScaler.\n",
        "Data Splitting: The data was split into training and testing sets with a 80/20 ratio.\n",
        "\n",
        "Model Training and Evaluation:\n",
        "\n",
        "1. Artificial Neural Network (ANN):\n",
        "\n",
        "Architecture: A Sequential model with two Dense layers (64 and 32 units, ReLU activation) and Dropout layers (0.3) for regularization, followed by a final Dense layer with sigmoid activation for binary classification.\n",
        "Training: Trained for 10 epochs with a batch size of 32, using Adam optimizer and binary cross-entropy loss. Early stopping was implemented to prevent overfitting.\n",
        "Performance (on test set):\n",
        "Accuracy: 0.9590\n",
        "Precision: 0.7589\n",
        "Recall: 0.1441\n",
        "F1-Score: 0.2422\n",
        "ROC-AUC: 0.5710\n",
        "\n",
        "\n",
        "2. 1D Convolutional Neural Network (1D CNN):\n",
        "\n",
        "Architecture: A Sequential model with two Conv1D blocks (128 and 64 filters, ReLU activation), Batch Normalization, MaxPooling1D, and Dropout layers (0.3). This is followed by a Flatten layer, a Dense layer (64 units, ReLU activation) with Dropout (0.3), and a final Dense layer with sigmoid activation.\n",
        "Training: Training was initiated for 100 epochs (interrupted during execution), with a batch size of 32, Adam optimizer, binary cross-entropy loss, and Early Stopping.\n",
        "Performance (on test set):\n",
        "Accuracy: 0.9584\n",
        "Precision: 0.8383\n",
        "Recall: 0.1047\n",
        "F1-Score: 0.1862\n",
        "ROC-AUC: 0.5519\n",
        "Comparison:\n",
        "\n",
        "\n",
        "\n",
        "Both models achieved high accuracy (around 95.8-95.9%), which is expected given the imbalanced dataset (low churn rate).\n",
        "The ANN model showed better Recall (0.1441) and F1-Score (0.2422) compared to the 1D CNN (Recall: 0.1047, F1-Score: 0.1862) for identifying churned customers.\n",
        "The CNN had slightly higher Precision (0.8383 vs 0.7589), meaning when it predicted churn, it was more often correct.\n",
        "The ROC-AUC scores for both models were relatively low (around 0.55-0.57), indicating that while they are good at predicting non-churn, their ability to distinguish between churn and non-churn cases across all thresholds is not very strong.\n",
        "The bar chart visually summarized these performance metrics, highlighting the trade-offs between the two models on different metrics. The dataset's class imbalance (only ~4.57% churn) suggests that metrics like Recall and F1-Score are more critical than overall accuracy for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaJrbnEx4Cn3"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "9054e5812adb29eebbcd6b680e8ef1afc4fe6e00a75ff130e735bd95b5b32301"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
